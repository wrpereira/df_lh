{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP INICIAL DO PROJETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#importação das bibliotecase e pacotes necessários para a análise\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas_gbq as gbq\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery_storage import BigQueryReadClient\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Carrega o .env: onde estão as credenciais do projeto/repositório\n",
    "load_dotenv(\"/mnt/c/Users/wrpen/OneDrive/Desktop/df_lh/.env\")\n",
    "\n",
    "# Detectar ambiente: como eu estou usando wsl-ubuntu, no VS Code  -  Windows, estava dando conflitos de path\n",
    "if os.name == \"nt\":  # se Windows\n",
    "    credentials_path = r\"C:\\Temp\\desafiolh-445818-3cb0f62cb9ef.json\"\n",
    "else:  # se WSL/Linux\n",
    "    credentials_path = \"/mnt/c/Temp/desafiolh-445818-3cb0f62cb9ef.json\"\n",
    "\n",
    "# Parâmetros injetados pelo Papermill ou definidos manualmente, caso não existam no ambiente\n",
    "# Tables_to_process: lista de tabelas que serão processadas\n",
    "# Output_dataset: nome do dataset onde os dados processados serão armazenados, neste caso, raw_data_cleaned\n",
    "if 'tables_to_process' not in locals():\n",
    "    tables_to_process = [\n",
    "        \"desafioadventureworks-446600.raw_data.production-product\"       \n",
    "    ]\n",
    "\n",
    "if 'output_dataset' not in locals():\n",
    "    output_dataset = \"desafioadventureworks-446600.raw_data_cleaned\"\n",
    "\n",
    "# Configs do cliente BigQuery: input de project e location de acordo com dados no Bigquery\n",
    "credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "client = bigquery.Client(credentials=credentials, project=os.getenv(\"BIGQUERY_PROJECT\"), location=\"us-central1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print com a tabela que vai ser processada nesse notebook\n",
    "\n",
    "print(\"Tabelas a processar:\", tables_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome do dataset no Bigquery com os dados brutos (.csv) extraídos pelo Meltano \n",
    "dataset_id = 'raw_data'\n",
    "print(dataset_id)\n",
    "\n",
    "# Lista de tabelas do dataset raw_data no Bigquery\n",
    "tables = client.list_tables('raw_data')\n",
    "print(\"Tabelas disponíveis:\")\n",
    "for table in tables:\n",
    "    print(table.table_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) e Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glossário dos dados:\n",
    "\n",
    "O termo ''doc:'', situado no rodapé de algumas cells, indica algo como:\n",
    "\n",
    "- documentação: documentar decisões, análises e resultados;\n",
    "\n",
    "- abreviações de termos, como bkp, df, entre outros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração para que o df exiba todas as colunas e todas as linhas completas, e também, exiba o formato numérico com 2 dígitos após a vírgula\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 10000)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "\n",
    "#doc: df = dataframe  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário para armazenar os df processados\n",
    "df_processados = {}\n",
    "\n",
    "# Iteração das tabelas e armazenamento em df\n",
    "for input_table in tables_to_process:\n",
    "    print(f\"Processando tabela: {input_table}\")\n",
    "    \n",
    "    # Nome da tabela com substituição de '-' por '_'\n",
    "    table_name = input_table.split(\".\")[-1].replace(\"-\", \"_\")  \n",
    "    \n",
    "    # Ler os dados da tabela do BigQuery para um df\n",
    "    print(\"Lendo os dados do BigQuery...\")\n",
    "    query = f\"SELECT * FROM `{input_table}`\"\n",
    "    table_data = client.query(query).to_dataframe()\n",
    "    \n",
    "    # Armazenar o df no dicionário\n",
    "    df_processados[table_name] = table_data\n",
    "    print(f\"Tabela {table_name} processada e armazenada com sucesso.\")\n",
    "\n",
    "# Print de validação\n",
    "print(\"Todas as tabelas foram processadas com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar todas as variáveis criadas dinamicamente\n",
    "for table_name in df_processados.keys():\n",
    "    print(f\"Variável criada: {table_name}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atribuir o df a uma variável com nome mais simples\n",
    "production_product = df_processados['production_product']\n",
    "\n",
    "print(f\"Colunas: {production_product.shape[1]}\\nLinhas: {production_product.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar duplicadas com base em 'productid'\n",
    "duplicadas = production_product[\n",
    "    production_product.duplicated(subset=['productid'], keep=False)\n",
    "]\n",
    "\n",
    "# Verificar se existem duplicadas\n",
    "if not duplicadas.empty:\n",
    "    # Ordenar duplicadas por 'productid' e 'modifieddate'\n",
    "    duplicadas_ordenadas = duplicadas.sort_values(by=['productid', 'modifieddate'])\n",
    "\n",
    "    # Exibir duplicadas ordenadas\n",
    "    print(\"duplicadas ordenadas:\")\n",
    "    print(duplicadas_ordenadas)\n",
    "else:\n",
    "    print(\"Não foram encontradas duplicadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover duplicadas mantendo a última ocorrência com base em 'modifieddate', pois ela que indica a data da última modificação nos dados\n",
    "# Importante, pois se houver erro na ingestão (duplicação), mantém os dados integros.\n",
    "\n",
    "production_product = production_product.drop_duplicates(subset=['productid'], keep='last')\n",
    "\n",
    "print(f\"Linhas após remover duplicadas (baseando-se na última 'modifieddate'): {len(production_product)}\")\n",
    "\n",
    "#bkp dos dados brutos\n",
    "raw_data_bkp_2_sem_duplicadas = production_product.copy()\n",
    "\n",
    "\n",
    "#doc: bkp = backup (cópia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar e exibir o df por 'productid'\n",
    "production_product = production_product.sort_values(by=['productid'])\n",
    "\n",
    "print(production_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar por todas as colunas do df, para verificar valores ausentes\n",
    "\n",
    "# Verificar valores ausentes na coluna\n",
    "for column in production_product.columns:   \n",
    "    missing_rows = production_product[production_product[column].isnull()]\n",
    "    print(f\"Coluna '{column}': {missing_rows.shape[0]} linhas ausentes.\")\n",
    "    \n",
    "# Mostrar as primeiras linhas ausentes, se preciso for, limitar o head() para dar menos outputs ou limitar os outputs\n",
    "    if not missing_rows.empty:\n",
    "        print(f\"Exibindo as primeiras linhas com valores ausentes em '{column}':\")\n",
    "        print(missing_rows.head(), \"\\n\")\n",
    "    else:\n",
    "        print(f\"Nenhuma linha com valores ausentes em '{column}'.\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#doc: apesar de apresentar várias colunas com muitas linhas ausentes, não há a necessidade de tratá-las, pois não interferem negativamente à análise        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores únicos por coluna, para verificar se colunas como flags, normalmente booleanas, possuem apenas 1 ou 2 valores.\n",
    "\n",
    "valores_unicos = production_product.nunique(dropna=False)\n",
    "\n",
    "print(\"Valores únicos incluindo NaN:\")\n",
    "print(valores_unicos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se a coluna é toda nula\n",
    "is_all_null = production_product['discontinueddate'].isnull().all()\n",
    "if is_all_null:\n",
    "    print(\"A coluna 'discontinueddate' é completamente NULL.\")\n",
    "else:\n",
    "    print(\"A coluna 'discontinueddate' possui valores não nulos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover a coluna 'discontinueddate' por ser completamente NULL\n",
    "production_product.drop(columns=['discontinueddate'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar colunas binárias esperadas e verificar valores únicos e suas distribuições\n",
    "coluna_binaria = ['makeflag', 'finishedgoodsflag']\n",
    "\n",
    "for col in coluna_binaria:\n",
    "    unique_values = production_product[col].unique()\n",
    "    print(f\"Valores únicos em '{col}': {unique_values}\")\n",
    "    print(f\"Distribuição de '{col}':\")\n",
    "    print(production_product[col].value_counts())\n",
    "    print()\n",
    "\n",
    "\n",
    "#doc: garantir que colunas binárias contenham apenas valores esperados, no caso True ou False e identificar anomalias verificando os valores únicos em cada coluna; valores fora do padrão binário, facilitando a validação e correção\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificar informações do df\n",
    "production_product.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliando as variáveis qualitativas\n",
    "\n",
    "coluna_qualitativa = [\"color\", \"size\", \"sizeunitmeasurecode\", \"weightunitmeasurecode\", \"productline\", \"class\", \"style\"]\n",
    "\n",
    "for col in coluna_qualitativa:\n",
    "    counts = production_product[col].value_counts().nlargest(10)\n",
    "    percentages = (counts / production_product.shape[0] * 100).map(\"{:.2f}%\".format)\n",
    "    summary = pd.DataFrame({\"qtde.\": counts, \"%\": percentages})\n",
    "    print(f\"Análise da coluna '{col}':\")\n",
    "    print(summary)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "#doc: qdte. = quantidade\n",
    "#doc*: variáveis qualitativas são um tipo de variável estatística que representam características ou atributos dos dados, sem serem medidas numericamente no nosso caso, city, por exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configurar o estilo do gráfico para fundo escuro\n",
    "sns.set(style='darkgrid', rc={\"axes.facecolor\": \"black\", \"figure.facecolor\": \"black\"})\n",
    "\n",
    "coluna_qualitativa = [\"color\", \"size\", \"weightunitmeasurecode\", \"productline\", \"class\", \"style\"]\n",
    "\n",
    "for col in coluna_qualitativa:\n",
    "    counts = production_product[col].value_counts().nlargest(10)  # Top 10 categorias\n",
    "    percentages = (counts / production_product.shape[0] * 100).map(\"{:.2f}%\".format)\n",
    "    summary = pd.DataFrame({\"qtde.\": counts, \"%\": percentages})\n",
    "    \n",
    "    # Criar o gráfico\n",
    "    plt.figure(figsize=(7, 3))\n",
    "    ax = sns.barplot(x=summary[\"qtde.\"], y=summary.index, color=\"blue\")  \n",
    "    \n",
    "    # Títulos e rótulos\n",
    "    plt.title(f\"Distribuição de '{col}'\", color='white', fontsize=14)\n",
    "    plt.xlabel(\"Quantidade\", color='white', fontsize=12)\n",
    "    plt.ylabel(\"\", color='white', fontsize=12)\n",
    "    \n",
    "    # Ajustar cores e rótulos\n",
    "    ax.tick_params(axis='x', colors='white')\n",
    "    ax.tick_params(axis='y', colors='white')\n",
    "    \n",
    "    # Adicionar os valores na frente das barras\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%d', color='white', fontsize=10, padding=4)\n",
    "    \n",
    "    # Remover a grade\n",
    "    ax.grid(False)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificar informações do df\n",
    "production_product.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis quantitativas*: \n",
    "\n",
    "# Identificar colunas numéricas para análise de outliers\n",
    "numeric_columns = ['safetystocklevel', 'reorderpoint', 'standardcost', 'listprice', 'weight', 'daystomanufacture']\n",
    "\n",
    "# Estatísticas Descritivas das colunas numéricas**\n",
    "print(\"Estatísticas Descritivas:\")\n",
    "print(production_product[numeric_columns].describe())\n",
    "\n",
    "# Cálculo de limites para outliers (IQR)***\n",
    "for col in numeric_columns:\n",
    "    q1 = production_product[col].quantile(0.25)\n",
    "    q3 = production_product[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    # Exibir os limites\n",
    "    print(f\"\\nColuna: {col}\")\n",
    "    print(f\"Limite inferior: {lower_bound}, Limite superior: {upper_bound}\")\n",
    "    \n",
    "    # Detecção e Análise de Outliers****\n",
    "    outliers = production_product[(production_product[col] < lower_bound) | (production_product[col] > upper_bound)]\n",
    "    print(f\"Outliers detectados ({len(outliers)}):\")\n",
    "    print(outliers[[col]])\n",
    "\n",
    "\n",
    "\n",
    "#doc*: variáveis quantitativas são um tipo de dado que pode ser representado por números e medidas objetivas, no nosso caso, vacationhours, sickleavehours, por exemplo\n",
    "#doc**: realizar estatísticas descritivas para entender a centralidade e variação dos dados (valores médios, mínimos, máximos, etc.)\n",
    "#doc***: calcular limites para identificar outliers (valores extremos que podem indicar erros ou casos excepcionais nos dados)\n",
    "#doc****: verificar a existência de outliers para decidir ações como remoção, substituição ou tratamento, garantindo qualidade dos dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de variáveis com outliers\n",
    "variables_with_outliers = ['standardcost', 'listprice', 'weight', 'daystomanufacture']\n",
    "\n",
    "# Configurar o layout da grade\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))  # 2x2 grid\n",
    "axes = axes.flatten()  # Para facilitar o acesso aos eixos\n",
    "\n",
    "# Criar os gráficos\n",
    "for i, var in enumerate(variables_with_outliers):\n",
    "    ax = sns.boxplot(\n",
    "        y=production_product[var],\n",
    "        color=\"blue\",\n",
    "        boxprops={\"color\": \"blue\"}, whiskerprops={\"color\": \"blue\"}, capprops={\"color\": \"blue\"},\n",
    "        medianprops={\"color\": \"red\"}, flierprops={\"markerfacecolor\": \"green\", \"markersize\": 5},\n",
    "        ax=axes[i]\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f'Distribuição de {var}', color='white', fontsize=14)\n",
    "    ax.set_ylabel(var, color='white', fontsize=12)\n",
    "    ax.set_xlabel(\"\")\n",
    "    \n",
    "    # Ajustar as cores dos ticks\n",
    "    ax.tick_params(axis='y', colors='white')\n",
    "    ax.tick_params(axis='x', colors='white')\n",
    "    \n",
    "    # Remover a grade (opcional)\n",
    "    ax.grid(False)\n",
    "\n",
    "# Ajustar o layout para evitar sobreposição\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#doc*****: de acordo com os resultados, as colunas safetystocklevel e reorderpoint não há necessidade de tratamento, sem outliers\n",
    "#          standardcost: 55 outliers acima de 792.69, o que pode ser aceitos, pois podem efletir produtos premium ou com custos elevados de produção\n",
    "#          listprice: Também identificados 55 outliers acima de 1412.47. Isso é esperado, já que produtos com custo alto podem ter preços elevados\n",
    "#          daystomanufacture: detectados 97 outliers acima de 2.5 dias. Esses valores (ex.: 4 dias) podem ser válidos para produtos mais complexos ou com maior tempo de produção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificando os outliers de da coluna weight: valores muito elevados para os componentes\n",
    "\n",
    "# Calcular os limites para a coluna weight\n",
    "q1 = production_product['weight'].quantile(0.25)\n",
    "q3 = production_product['weight'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "# Filtrar valores que ultrapassam os limites\n",
    "outliers_weight = production_product[\n",
    "    (production_product['weight'] < lower_bound) | \n",
    "    (production_product['weight'] > upper_bound)\n",
    "]\n",
    "\n",
    "# Ordenar os outliers por weight em ordem decrescente\n",
    "outliers_weight_sorted = outliers_weight[['productid', 'name', 'weight']].sort_values(by='weight', ascending=False)\n",
    "\n",
    "# Exibir os valores\n",
    "print(outliers_weight_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorias de produtos com outliers em 'weight'\n",
    "categories_with_outliers = outliers_weight.groupby('name')['weight'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Exibir categorias com peso médio para revisão\n",
    "print(categories_with_outliers)\n",
    "\n",
    "\n",
    "#doc:uma lista de nomes de produtos (ex.: \"LL Road Rear Wheel\", \"ML Crankset\") e seus pesos médios; \n",
    "#    visão consolidada para validar quais categorias têm valores incoerentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir valores aproximados por categoria (em kg)\n",
    "weight_corrections = {\n",
    "    \"LL Road Rear Wheel\": 1.5,\n",
    "    \"ML Road Rear Wheel\": 1.5,\n",
    "    \"LL Road Front Wheel\": 1.2,\n",
    "    \"HL Road Rear Wheel\": 1.5,\n",
    "    \"ML Road Front Wheel\": 1.2,\n",
    "    \"HL Road Front Wheel\": 1.2,\n",
    "    \"ML Crankset\": 1.0,\n",
    "    \"LL Crankset\": 1.0,\n",
    "    \"HL Crankset\": 1.0,\n",
    "    \"Touring Rim\": 0.7,\n",
    "    \"ML Road Rim\": 0.7,\n",
    "    \"ML Mountain Rim\": 0.8,\n",
    "    \"LL Road Rim\": 0.7,\n",
    "    \"LL Mountain Rim\": 0.8,\n",
    "    \"HL Mountain Rim\": 0.8,\n",
    "    \"HL Road Rim\": 0.7,\n",
    "    \"Rear Brakes\": 0.3,\n",
    "    \"Front Brakes\": 0.3,\n",
    "    \"LL Bottom Bracket\": 0.4,\n",
    "    \"LL Mountain Pedal\": 0.4,\n",
    "    \"ML Mountain Pedal\": 0.4,\n",
    "    \"Rear Derailleur\": 0.5,\n",
    "    \"LL Road Pedal\": 0.3,\n",
    "    \"HL Mountain Pedal\": 0.4,\n",
    "    \"HL Bottom Bracket\": 0.4,\n",
    "    \"ML Bottom Bracket\": 0.4,\n",
    "    \"ML Road Pedal\": 0.3,\n",
    "    \"HL Road Pedal\": 0.3,\n",
    "    \"Front Derailleur\": 0.3,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar correções no DataFrame\n",
    "production_product['weight_corrected'] = production_product['name'].map(weight_corrections).fillna(production_product['weight'])\n",
    "\n",
    "# Verificar os resultados\n",
    "print(production_product[['name', 'weight', 'weight_corrected']].sort_values(by='weight', ascending=False).head(20))\n",
    "\n",
    "\n",
    "# Correção de valores inconsistentes na coluna 'weight'\n",
    "\n",
    "# 1. Identificação de inconsistências\n",
    "# Durante a análise, algumas categorias de produtos apresentavam valores de peso incoerentes com a realidade, como rodas com 1050 kg e cranksets com 635 kg. \n",
    "# Esses valores foram considerados erros de entrada de dados.\n",
    "\n",
    "# 2. Definição de pesos aproximados\n",
    "# Foi criado um dicionário chamado 'weight_corrections' contendo valores aproximados para cada\n",
    "# categoria de produto com base em características reais de produtos similares. Esses valores\n",
    "# representam estimativas coerentes dentro do domínio de produtos da Adventure Works.\n",
    "\n",
    "# 3. Aplicação das correções\n",
    "# Os valores inconsistentes foram substituídos pelos valores definidos no dicionário.\n",
    "# Para isso, a função 'map()' foi utilizada para associar os valores corrigidos às categorias\n",
    "# específicas, criando uma nova coluna chamada 'weight_corrected'.\n",
    "# Caso um produto não estivesse no dicionário de correções, o valor original foi mantido.\n",
    "\n",
    "# 4. Resultado\n",
    "# A nova coluna 'weight_corrected' foi criada, contendo os valores ajustados.\n",
    "# Agora, essa coluna pode ser utilizada para análises e exportação, enquanto a coluna original\n",
    "# 'weight' pode ser mantida para rastreamento e auditoria, se necessário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganizar as colunas no DataFrame para que weight_corrected fique ao lado de weight\n",
    "cols = production_product.columns.tolist()\n",
    "if 'weight' in cols and 'weight_corrected' in cols:\n",
    "    weight_index = cols.index('weight')\n",
    "    cols.insert(weight_index + 1, cols.pop(cols.index('weight_corrected')))\n",
    "production_product = production_product[cols]\n",
    "\n",
    "# Verificar as colunas reorganizadas\n",
    "print(production_product.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualizar o dicionário df_processados com o df ajustado\n",
    "df_processados['production_product'] = production_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padronizar colunas com valores textuais\n",
    "production_product['name'] = production_product['name'].str.strip().str.upper()\n",
    "production_product['productnumber'] = production_product['productnumber'].str.strip().str.upper()\n",
    "production_product['color'] = production_product['color'].str.strip().str.upper()\n",
    "production_product['weightunitmeasurecode'] = production_product['weightunitmeasurecode'].str.strip().str.upper()\n",
    "production_product['productline'] = production_product['productline'].str.strip().str.upper()\n",
    "production_product['class'] = production_product['class'].str.strip().str.upper()\n",
    "production_product['size'] = production_product['size'].str.strip().str.upper()\n",
    "production_product['rowguid'] = production_product['rowguid'].str.strip().str.upper()\n",
    "\n",
    "print(production_product.head())\n",
    "\n",
    "#doc: padronizar as strings nessa etapa, contribui para a execução das demais etapas do pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Garantir que apenas tabelas únicas sejam exportadas\n",
    "unique_df_processados = {k: v for k, v in df_processados.items()}\n",
    "\n",
    "# Exportar tabelas para o BigQuery\n",
    "for table_name, df_cleaned in unique_df_processados.items():\n",
    "    # Nome da tabela no BigQuery\n",
    "    output_table = f\"{output_dataset}.{table_name}\"\n",
    "\n",
    "    # Configurar job de exportação\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_TRUNCATE\"  \n",
    "    )\n",
    "    \n",
    "    # Exportar DataFrame para o BigQuery\n",
    "    job = client.load_table_from_dataframe(df_cleaned, output_table, job_config=job_config)\n",
    "    job.result()\n",
    "\n",
    "    print(f\"Tabela {table_name} exportada com sucesso para {output_table}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
