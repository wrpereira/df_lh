{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP INICIAL DO PROJETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#importação das bibliotecase e pacotes necessários para a análise\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas_gbq as gbq\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery_storage import BigQueryReadClient\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Carrega o .env: onde estão as credenciais do projeto/repositório\n",
    "load_dotenv(\"/mnt/c/Users/wrpen/OneDrive/Desktop/df_lh/.env\")\n",
    "\n",
    "# Detectar ambiente: como eu estou usando wsl-ubuntu, no VS Code  -  Windows, estava dando conflitos de path\n",
    "if os.name == \"nt\":  # se Windows\n",
    "    credentials_path = r\"C:\\Temp\\desafiolh-445818-3cb0f62cb9ef.json\"\n",
    "else:  # se WSL/Linux\n",
    "    credentials_path = \"/mnt/c/Temp/desafiolh-445818-3cb0f62cb9ef.json\"\n",
    "\n",
    "# Parâmetros injetados pelo Papermill ou definidos manualmente, caso não existam no ambiente\n",
    "# Tables_to_process: lista de tabelas que serão processadas\n",
    "# Output_dataset: nome do dataset onde os dados processados serão armazenados, neste caso, raw_data_cleaned\n",
    "if 'tables_to_process' not in locals():\n",
    "    tables_to_process = [\n",
    "        \"desafioadventureworks-446600.raw_data.sales-customer\"       \n",
    "    ]\n",
    "\n",
    "if 'output_dataset' not in locals():\n",
    "    output_dataset = \"desafioadventureworks-446600.raw_data_cleaned\"\n",
    "\n",
    "# Configs do cliente BigQuery: input de project e location de acordo com dados no Bigquery\n",
    "credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "client = bigquery.Client(credentials=credentials, project=os.getenv(\"BIGQUERY_PROJECT\"), location=\"us-central1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabelas a processar: ['desafioadventureworks-446600.raw_data.sales-customer']\n"
     ]
    }
   ],
   "source": [
    "# Print com a tabela que vai ser processada nesse notebook\n",
    "\n",
    "print(\"Tabelas a processar:\", tables_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_data\n",
      "Tabelas disponíveis:\n",
      "humanresources-employee\n",
      "person-address\n",
      "person-businessentity\n",
      "person-person\n",
      "person-stateprovince\n",
      "production-location\n",
      "production-product\n",
      "production-productcategory\n",
      "production-productinventory\n",
      "production-productsubcategory\n",
      "sales-customer\n",
      "sales-salesorderdetail\n",
      "sales-salesorderheader\n",
      "sales-salesterritory\n",
      "sales-store\n"
     ]
    }
   ],
   "source": [
    "# Nome do dataset no Bigquery com os dados brutos (.csv) extraídos pelo Meltano \n",
    "dataset_id = 'raw_data'\n",
    "print(dataset_id)\n",
    "\n",
    "# Lista de tabelas do dataset raw_data no Bigquery\n",
    "tables = client.list_tables('raw_data')\n",
    "print(\"Tabelas disponíveis:\")\n",
    "for table in tables:\n",
    "    print(table.table_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) e Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glossário dos dados:\n",
    "\n",
    "O termo ''doc:'', situado no rodapé de algumas cells, indica algo como:\n",
    "\n",
    "- documentação: documentar decisões, análises e resultados;\n",
    "\n",
    "- abreviações de termos, como bkp, df, entre outros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração para que o df exiba todas as colunas e todas as linhas completas, e também, exiba o formato numérico com 2 dígitos após a vírgula\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 10000)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "\n",
    "#doc: df = dataframe  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando tabela: desafioadventureworks-446600.raw_data.sales-customer\n",
      "Lendo os dados do BigQuery...\n",
      "Tabela sales_customer processada e armazenada com sucesso.\n",
      "Todas as tabelas foram processadas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Dicionário para armazenar os df processados\n",
    "df_processados = {}\n",
    "\n",
    "# Iteração das tabelas e armazenamento em df\n",
    "for input_table in tables_to_process:\n",
    "    print(f\"Processando tabela: {input_table}\")\n",
    "    \n",
    "    # Nome da tabela com substituição de '-' por '_'\n",
    "    table_name = input_table.split(\".\")[-1].replace(\"-\", \"_\")  \n",
    "    \n",
    "    # Ler os dados da tabela do BigQuery para um df\n",
    "    print(\"Lendo os dados do BigQuery...\")\n",
    "    query = f\"SELECT * FROM `{input_table}`\"\n",
    "    table_data = client.query(query).to_dataframe()\n",
    "    \n",
    "    # Armazenar o df no dicionário\n",
    "    df_processados[table_name] = table_data\n",
    "    print(f\"Tabela {table_name} processada e armazenada com sucesso.\")\n",
    "\n",
    "# Print de validação\n",
    "print(\"Todas as tabelas foram processadas com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variável criada: sales_customer\n"
     ]
    }
   ],
   "source": [
    "# Listar todas as variáveis criadas dinamicamente\n",
    "for table_name in df_processados.keys():\n",
    "    print(f\"Variável criada: {table_name}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas: 6\n",
      "Linhas: 317120\n"
     ]
    }
   ],
   "source": [
    "# Atribuir o df a uma variável com nome mais simples\n",
    "sales_customer = df_processados['sales_customer']\n",
    "\n",
    "print(f\"Colunas: {sales_customer.shape[1]}\\nLinhas: {sales_customer.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicadas ordenadas:\n",
      "        customerid  personid  storeid  territoryid                               rowguid                     modifieddate\n",
      "0                1      <NA>      934            1  3f5ae95e-b87d-4aed-95b4-c3797afcb74f 2014-09-12 11:15:07.263000+00:00\n",
      "19820            1      <NA>      934            1  3f5ae95e-b87d-4aed-95b4-c3797afcb74f 2014-09-12 11:15:07.263000+00:00\n",
      "39640            1      <NA>      934            1  3f5ae95e-b87d-4aed-95b4-c3797afcb74f 2014-09-12 11:15:07.263000+00:00\n",
      "59460            1      <NA>      934            1  3f5ae95e-b87d-4aed-95b4-c3797afcb74f 2014-09-12 11:15:07.263000+00:00\n",
      "79280            1      <NA>      934            1  3f5ae95e-b87d-4aed-95b4-c3797afcb74f 2014-09-12 11:15:07.263000+00:00\n",
      "...            ...       ...      ...          ...                                   ...                              ...\n",
      "221784       30118      1993     1994            3  2495b4eb-fe8b-459e-a1b6-dba25c04e626 2014-09-12 11:15:07.263000+00:00\n",
      "241604       30118      1993     1994            3  2495b4eb-fe8b-459e-a1b6-dba25c04e626 2014-09-12 11:15:07.263000+00:00\n",
      "261424       30118      1993     1994            3  2495b4eb-fe8b-459e-a1b6-dba25c04e626 2014-09-12 11:15:07.263000+00:00\n",
      "281244       30118      1993     1994            3  2495b4eb-fe8b-459e-a1b6-dba25c04e626 2014-09-12 11:15:07.263000+00:00\n",
      "301064       30118      1993     1994            3  2495b4eb-fe8b-459e-a1b6-dba25c04e626 2014-09-12 11:15:07.263000+00:00\n",
      "\n",
      "[317120 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Identificar duplicadas com base em 'customerid'\n",
    "duplicadas = sales_customer[\n",
    "    sales_customer.duplicated(subset=['customerid'], keep=False)\n",
    "]\n",
    "\n",
    "# Verificar se existem duplicadas\n",
    "if not duplicadas.empty:\n",
    "    # Ordenar duplicadas por 'customerid' e 'modifieddate'\n",
    "    duplicadas_ordenadas = duplicadas.sort_values(by=['customerid', 'modifieddate'])\n",
    "\n",
    "    # Exibir duplicadas ordenadas\n",
    "    print(\"duplicadas ordenadas:\")\n",
    "    print(duplicadas_ordenadas)\n",
    "else:\n",
    "    print(\"Não foram encontradas duplicadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linhas após remover duplicadas (baseando-se na última 'modifieddate'): 19820\n"
     ]
    }
   ],
   "source": [
    "# Remover duplicadas mantendo a última ocorrência com base em 'modifieddate', pois ela que indica a data da última modificação nos dados\n",
    "# Importante, pois se houver erro na ingestão (duplicação), mantém os dados integros.\n",
    "\n",
    "sales_customer = sales_customer.drop_duplicates(subset=['customerid'], keep='last')\n",
    "\n",
    "print(f\"Linhas após remover duplicadas (baseando-se na última 'modifieddate'): {len(sales_customer)}\")\n",
    "\n",
    "#bkp dos dados brutos\n",
    "raw_data_bkp_2_sem_duplicadas = sales_customer.copy()\n",
    "\n",
    "\n",
    "#doc: bkp = backup (cópia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        customerid  personid  storeid  territoryid                               rowguid                     modifieddate\n",
      "297300           1      <NA>      934            1  3f5ae95e-b87d-4aed-95b4-c3797afcb74f 2014-09-12 11:15:07.263000+00:00\n",
      "297301           2      <NA>     1028            1  e552f657-a9af-4a7d-a645-c429d6e02491 2014-09-12 11:15:07.263000+00:00\n",
      "301065           3      <NA>      642            4  130774b1-db21-4ef3-98c8-c104bcd6ed6d 2014-09-12 11:15:07.263000+00:00\n",
      "301066           4      <NA>      932            4  ff862851-1daa-4044-be7c-3e85583c054d 2014-09-12 11:15:07.263000+00:00\n",
      "301067           5      <NA>     1026            4  83905bdc-6f5e-4f71-b162-c98da069f38a 2014-09-12 11:15:07.263000+00:00\n",
      "...            ...       ...      ...          ...                                   ...                              ...\n",
      "309611       30114      1985     1986            7  97154f3d-28f1-4b15-ae03-9518b781ece3 2014-09-12 11:15:07.263000+00:00\n",
      "307727       30115      1987     1988            6  e4cf8fd5-30a4-4b8e-8fd8-47032e255778 2014-09-12 11:15:07.263000+00:00\n",
      "305759       30116      1989     1990            4  ec409609-d25d-41b8-9d15-a1aa6e89fc77 2014-09-12 11:15:07.263000+00:00\n",
      "305760       30117      1991     1992            4  6f08e2fb-1cd3-4f6e-a2e6-385669598b19 2014-09-12 11:15:07.263000+00:00\n",
      "301064       30118      1993     1994            3  2495b4eb-fe8b-459e-a1b6-dba25c04e626 2014-09-12 11:15:07.263000+00:00\n",
      "\n",
      "[19820 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Ordenar e exibir o df por 'customerid'\n",
    "sales_customer = sales_customer.sort_values(by=['customerid'])\n",
    "\n",
    "print(sales_customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coluna 'customerid': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'customerid'.\n",
      "\n",
      "Coluna 'personid': 701 linhas ausentes.\n",
      "Exibindo as primeiras linhas com valores ausentes em 'personid':\n",
      "        customerid  personid  storeid  territoryid                               rowguid                     modifieddate\n",
      "297300           1      <NA>      934            1  3f5ae95e-b87d-4aed-95b4-c3797afcb74f 2014-09-12 11:15:07.263000+00:00\n",
      "297301           2      <NA>     1028            1  e552f657-a9af-4a7d-a645-c429d6e02491 2014-09-12 11:15:07.263000+00:00\n",
      "301065           3      <NA>      642            4  130774b1-db21-4ef3-98c8-c104bcd6ed6d 2014-09-12 11:15:07.263000+00:00\n",
      "301066           4      <NA>      932            4  ff862851-1daa-4044-be7c-3e85583c054d 2014-09-12 11:15:07.263000+00:00\n",
      "301067           5      <NA>     1026            4  83905bdc-6f5e-4f71-b162-c98da069f38a 2014-09-12 11:15:07.263000+00:00 \n",
      "\n",
      "Coluna 'storeid': 18484 linhas ausentes.\n",
      "Exibindo as primeiras linhas com valores ausentes em 'storeid':\n",
      "        customerid  personid  storeid  territoryid                               rowguid                     modifieddate\n",
      "311504       11000     13531     <NA>            9  477586b3-2977-4e54-b1a8-569ab2c7c4d4 2014-09-12 11:15:07.263000+00:00\n",
      "311505       11001      5454     <NA>            9  c32a8084-9077-4f13-9738-1e2da7c1dcd9 2014-09-12 11:15:07.263000+00:00\n",
      "311506       11002     11269     <NA>            9  45715dd8-2f57-4a39-beb4-6a8f99d59794 2014-09-12 11:15:07.263000+00:00\n",
      "311507       11003     11358     <NA>            9  7e240efc-7ee6-4814-93a8-269821157e18 2014-09-12 11:15:07.263000+00:00\n",
      "311508       11004     11901     <NA>            9  61ccb4d0-2328-4bbb-aef9-e7e0b0fdd67a 2014-09-12 11:15:07.263000+00:00 \n",
      "\n",
      "Coluna 'territoryid': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'territoryid'.\n",
      "\n",
      "Coluna 'rowguid': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'rowguid'.\n",
      "\n",
      "Coluna 'modifieddate': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'modifieddate'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterar por todas as colunas do df, para verificar valores ausentes\n",
    "\n",
    "# Verificar valores ausentes na coluna\n",
    "for column in sales_customer.columns:   \n",
    "    missing_rows = sales_customer[sales_customer[column].isnull()]\n",
    "    print(f\"Coluna '{column}': {missing_rows.shape[0]} linhas ausentes.\")\n",
    "    \n",
    "# Mostrar as primeiras linhas ausentes, se preciso for, limitar o head() para dar menos outputs ou limitar os outputs\n",
    "    if not missing_rows.empty:\n",
    "        print(f\"Exibindo as primeiras linhas com valores ausentes em '{column}':\")\n",
    "        print(missing_rows.head(), \"\\n\")\n",
    "    else:\n",
    "        print(f\"Nenhuma linha com valores ausentes em '{column}'.\\n\")\n",
    "\n",
    "\n",
    "#doc: as colunas personid e storeid possuem muitos valores ausentes, mas manter essas colunas ainda fazem sentido, se tratado de ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores únicos incluindo NaN:\n",
      "customerid      19820\n",
      "personid        19120\n",
      "storeid           702\n",
      "territoryid        10\n",
      "rowguid         19820\n",
      "modifieddate        1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Valores únicos por coluna, para verificar se colunas como flags, normalmente booleanas, possuem apenas 1 ou 2 valores.\n",
    "\n",
    "valores_unicos = sales_customer.nunique(dropna=False)\n",
    "\n",
    "print(\"Valores únicos incluindo NaN:\")\n",
    "print(valores_unicos)\n",
    "\n",
    "#doc: currentflag possue somente 1 valor, o que indica que pode ser somente valores True ou False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19820 entries, 297300 to 301064\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype              \n",
      "---  ------        --------------  -----              \n",
      " 0   customerid    19820 non-null  Int64              \n",
      " 1   personid      19119 non-null  Int64              \n",
      " 2   storeid       1336 non-null   Int64              \n",
      " 3   territoryid   19820 non-null  Int64              \n",
      " 4   rowguid       19820 non-null  object             \n",
      " 5   modifieddate  19820 non-null  datetime64[us, UTC]\n",
      "dtypes: Int64(4), datetime64[us, UTC](1), object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#verificar informações do df\n",
    "sales_customer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualizar o dicionário df_processados com o df ajustado\n",
    "df_processados['sales_customer'] = sales_customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        customerid  personid  storeid  territoryid                               rowguid                     modifieddate\n",
      "297300           1      <NA>      934            1  3F5AE95E-B87D-4AED-95B4-C3797AFCB74F 2014-09-12 11:15:07.263000+00:00\n",
      "297301           2      <NA>     1028            1  E552F657-A9AF-4A7D-A645-C429D6E02491 2014-09-12 11:15:07.263000+00:00\n",
      "301065           3      <NA>      642            4  130774B1-DB21-4EF3-98C8-C104BCD6ED6D 2014-09-12 11:15:07.263000+00:00\n",
      "301066           4      <NA>      932            4  FF862851-1DAA-4044-BE7C-3E85583C054D 2014-09-12 11:15:07.263000+00:00\n",
      "301067           5      <NA>     1026            4  83905BDC-6F5E-4F71-B162-C98DA069F38A 2014-09-12 11:15:07.263000+00:00\n"
     ]
    }
   ],
   "source": [
    "# Padronizar colunas com valores textuais\n",
    "sales_customer['rowguid'] = sales_customer['rowguid'].str.strip().str.upper()\n",
    "\n",
    "print(sales_customer.head())\n",
    "\n",
    "#doc: padronizar as strings nessa etapa, contribui para a execução das demais etapas do pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela sales_customer exportada com sucesso para desafioadventureworks-446600.raw_data_cleaned.sales_customer.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Garantir que apenas tabelas únicas sejam exportadas\n",
    "unique_df_processados = {k: v for k, v in df_processados.items()}\n",
    "\n",
    "# Exportar tabelas para o BigQuery\n",
    "for table_name, df_cleaned in unique_df_processados.items():\n",
    "    # Nome da tabela no BigQuery\n",
    "    output_table = f\"{output_dataset}.{table_name}\"\n",
    "\n",
    "    # Configurar job de exportação\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_TRUNCATE\"  \n",
    "    )\n",
    "    \n",
    "    # Exportar DataFrame para o BigQuery\n",
    "    job = client.load_table_from_dataframe(df_cleaned, output_table, job_config=job_config)\n",
    "    job.result()\n",
    "\n",
    "    print(f\"Tabela {table_name} exportada com sucesso para {output_table}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
