{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP INICIAL DO PROJETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#importação das bibliotecase e pacotes necessários para a análise\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas_gbq as gbq\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery_storage import BigQueryReadClient\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Carrega o .env: onde estão as credenciais do projeto/repositório\n",
    "load_dotenv(\"/mnt/c/Users/wrpen/OneDrive/Desktop/df_lh/.env\")\n",
    "\n",
    "# Detectar ambiente: como eu estou usando wsl-ubuntu, no VS Code  -  Windows, estava dando conflitos de path\n",
    "if os.name == \"nt\":  # se Windows\n",
    "    credentials_path = r\"C:\\Temp\\desafiolh-445818-3cb0f62cb9ef.json\"\n",
    "else:  # se WSL/Linux\n",
    "    credentials_path = \"/mnt/c/Temp/desafiolh-445818-3cb0f62cb9ef.json\"\n",
    "\n",
    "# Parâmetros injetados pelo Papermill ou definidos manualmente, caso não existam no ambiente\n",
    "# Tables_to_process: lista de tabelas que serão processadas\n",
    "# Output_dataset: nome do dataset onde os dados processados serão armazenados, neste caso, raw_data_cleaned\n",
    "if 'tables_to_process' not in locals():\n",
    "    tables_to_process = [\n",
    "        \"desafioadventureworks-446600.raw_data.sales-salesterritory\"       \n",
    "    ]\n",
    "\n",
    "if 'output_dataset' not in locals():\n",
    "    output_dataset = \"desafioadventureworks-446600.raw_data_cleaned\"\n",
    "\n",
    "# Configs do cliente BigQuery: input de project e location de acordo com dados no Bigquery\n",
    "credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "client = bigquery.Client(credentials=credentials, project=os.getenv(\"BIGQUERY_PROJECT\"), location=\"us-central1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print com a tabela que vai ser processada nesse notebook\n",
    "\n",
    "print(\"Tabelas a processar:\", tables_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome do dataset no Bigquery com os dados brutos (.csv) extraídos pelo Meltano \n",
    "dataset_id = 'raw_data'\n",
    "print(dataset_id)\n",
    "\n",
    "# Lista de tabelas do dataset raw_data no Bigquery\n",
    "tables = client.list_tables('raw_data')\n",
    "print(\"Tabelas disponíveis:\")\n",
    "for table in tables:\n",
    "    print(table.table_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) e Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glossário dos dados:\n",
    "\n",
    "O termo ''doc:'', situado no rodapé de algumas cells, indica algo como:\n",
    "\n",
    "- documentação: documentar decisões, análises e resultados;\n",
    "\n",
    "- abreviações de termos, como bkp, df, entre outros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração para que o df exiba todas as colunas e todas as linhas completas, e também, exiba o formato numérico com 2 dígitos após a vírgula\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 10000)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "\n",
    "#doc: df = dataframe  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário para armazenar os df processados\n",
    "df_processados = {}\n",
    "\n",
    "# Iteração das tabelas e armazenamento em df\n",
    "for input_table in tables_to_process:\n",
    "    print(f\"Processando tabela: {input_table}\")\n",
    "    \n",
    "    # Nome da tabela com substituição de '-' por '_'\n",
    "    table_name = input_table.split(\".\")[-1].replace(\"-\", \"_\")  \n",
    "    \n",
    "    # Ler os dados da tabela do BigQuery para um df\n",
    "    print(\"Lendo os dados do BigQuery...\")\n",
    "    query = f\"SELECT * FROM `{input_table}`\"\n",
    "    table_data = client.query(query).to_dataframe()\n",
    "    \n",
    "    # Armazenar o df no dicionário\n",
    "    df_processados[table_name] = table_data\n",
    "    print(f\"Tabela {table_name} processada e armazenada com sucesso.\")\n",
    "\n",
    "# Print de validação\n",
    "print(\"Todas as tabelas foram processadas com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar todas as variáveis criadas dinamicamente\n",
    "for table_name in df_processados.keys():\n",
    "    print(f\"Variável criada: {table_name}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atribuir o df a uma variável com nome mais simples\n",
    "sales_salesterritory = df_processados['sales_salesterritory']\n",
    "\n",
    "print(f\"Colunas: {sales_salesterritory.shape[1]}\\nLinhas: {sales_salesterritory.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar duplicadas com base em 'territoryid'\n",
    "duplicadas = sales_salesterritory[\n",
    "    sales_salesterritory.duplicated(subset=['territoryid'], keep=False)\n",
    "]\n",
    "\n",
    "# Verificar se existem duplicadas\n",
    "if not duplicadas.empty:\n",
    "    # Ordenar duplicadas por 'territoryid' e 'modifieddate'\n",
    "    duplicadas_ordenadas = duplicadas.sort_values(by=['territoryid', 'modifieddate'])\n",
    "\n",
    "    # Exibir duplicadas ordenadas\n",
    "    print(\"duplicadas ordenadas:\")\n",
    "    print(duplicadas_ordenadas)\n",
    "else:\n",
    "    print(\"Não foram encontradas duplicadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover duplicadas mantendo a última ocorrência com base em 'modifieddate', pois ela que indica a data da última modificação nos dados\n",
    "# Importante, pois se houver erro na ingestão (duplicação), mantém os dados integros.\n",
    "\n",
    "sales_salesterritory = sales_salesterritory.drop_duplicates(subset=['territoryid'], keep='last')\n",
    "\n",
    "print(f\"Linhas após remover duplicadas (baseando-se na última 'modifieddate'): {len(sales_salesterritory)}\")\n",
    "\n",
    "#bkp dos dados brutos\n",
    "raw_data_bkp_2_sem_duplicadas = sales_salesterritory.copy()\n",
    "\n",
    "\n",
    "#doc: bkp = backup (cópia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar e exibir o df por 'territoryid'\n",
    "sales_salesterritory = sales_salesterritory.sort_values(by=['territoryid'])\n",
    "\n",
    "print(sales_salesterritory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar por todas as colunas do df, para verificar valores ausentes\n",
    "\n",
    "# Verificar valores ausentes na coluna\n",
    "for column in sales_salesterritory.columns:   \n",
    "    missing_rows = sales_salesterritory[sales_salesterritory[column].isnull()]\n",
    "    print(f\"Coluna '{column}': {missing_rows.shape[0]} linhas ausentes.\")\n",
    "    \n",
    "# Mostrar as primeiras linhas ausentes, se preciso for, limitar o head() para dar menos outputs ou limitar os outputs\n",
    "    if not missing_rows.empty:\n",
    "        print(f\"Exibindo as primeiras linhas com valores ausentes em '{column}':\")\n",
    "        print(missing_rows.head(), \"\\n\")\n",
    "    else:\n",
    "        print(f\"Nenhuma linha com valores ausentes em '{column}'.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores únicos por coluna, para verificar se colunas como flags, normalmente booleanas, possuem apenas 1 ou 2 valores.\n",
    "\n",
    "valores_unicos = sales_salesterritory.nunique(dropna=False)\n",
    "\n",
    "print(\"Valores únicos incluindo NaN:\")\n",
    "print(valores_unicos)\n",
    "\n",
    "#doc: currentflag possue somente 1 valor, o que indica que pode ser somente valores True ou False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar colunas com apenas 1 valor único (incluindo NaN)\n",
    "colunas_com_1_valor = valores_unicos[valores_unicos == 1].index\n",
    "\n",
    "# Exibir os valores únicos dessas colunas\n",
    "print(\"Colunas com apenas 1 valor único:\")\n",
    "for col in colunas_com_1_valor:\n",
    "    valor_unico = sales_salesterritory[col].unique()\n",
    "    print(f\"{col}: {valor_unico}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar colunas com valor único igual a 0\n",
    "colunas_para_remover = [col for col in sales_salesterritory.columns if sales_salesterritory[col].nunique() == 1 and sales_salesterritory[col].unique()[0] == 0]\n",
    "\n",
    "# Remover as colunas identificadas\n",
    "sales_salesterritory = sales_salesterritory.drop(columns=colunas_para_remover)\n",
    "\n",
    "# Exibir o DataFrame atualizado e as colunas restantes\n",
    "print(\"Colunas restantes no DataFrame:\")\n",
    "print(sales_salesterritory.columns.tolist())\n",
    "\n",
    "# Visualizar os dados atualizados\n",
    "print(\"\\nDataFrame atualizado:\")\n",
    "print(sales_salesterritory)\n",
    "\n",
    "\n",
    "#doc*: as colunas costytf e Length apresentaram todas as linhas com o valor 0, portanto, sem valor agregado as análises - colunas deletadas\n",
    "#      Mantive a coluna modifieddate, por se tratar de datas e ser fundamental em análises futuras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_salesterritory.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avaliando as variáveis qualitativas*\n",
    "\n",
    "coluna_quantitativa = [\"name\", \"countryregioncode\", \"group\"]\n",
    "for col in coluna_quantitativa:\n",
    "    counts = sales_salesterritory[col].value_counts().nlargest(10)\n",
    "    percentages = (counts / sales_salesterritory.shape[0] * 100).map(\"{:.2f}%\".format)\n",
    "    summary = pd.DataFrame({\"qtde.\": counts, \"%\": percentages})\n",
    "    print(summary)    \n",
    "\n",
    "\n",
    "#doc*: variáveis qualitativas são um tipo de variável estatística que representam características ou atributos dos dados, sem serem medidas numericamente\n",
    "#      no nosso caso, name, countryregioncode, group, por exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dominância geográfica*\n",
    "\n",
    "group_counts = sales_salesterritory['group'].value_counts()\n",
    "\n",
    "\n",
    "sns.set(style='ticks', rc={\"axes.facecolor\": \"black\", \"figure.facecolor\": \"black\"})\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax = sns.barplot(x=group_counts.index, y=group_counts.values, palette='viridis')\n",
    "plt.title(\"Distribuição de Grupos\", color='white', fontsize=14)\n",
    "plt.xlabel(\"Grupo\", color='white', fontsize=12)\n",
    "plt.ylabel(\"\", fontsize=12)  \n",
    "plt.xticks(color='white', fontsize=10)\n",
    "ax.tick_params(axis='y', left=False, labelleft=False)\n",
    "ax.grid(False)\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%d', color='white', fontsize=10, label_type='edge')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#doc*: a região North America domina com 60% das ocorrências no grupo,\n",
    "#      o código de país US representa 50% dos dados e CA repsenta 10%  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alinhamento de grupos com países*\n",
    "\n",
    "group_country = sales_salesterritory.groupby(['group', 'countryregioncode']).size().unstack(fill_value=0)\n",
    "\n",
    "# Heatmap para visualização\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(group_country, annot=True, fmt='d', cmap='coolwarm', cbar=False)\n",
    "plt.title(\"Alinhamento de Grupos com Países\", color='white', fontsize=14)\n",
    "plt.xlabel(\"Código de País\", color='white', fontsize=12)\n",
    "plt.ylabel(\"Grupo\", color='white', fontsize=12)\n",
    "plt.xticks(color='white', fontsize=10)\n",
    "plt.yticks(color='white', fontsize=10)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#doc*: identificar quais países pertencem a quais grupos e se há algum desequilíbrio na distribuição (North America: US=5;CA=1)\n",
    "#      entender a relação entre grupos e países ajuda a otimizar operações regionais, ajustar estratégias de marketing e identificar áreas de oportunidades de crescimento.\n",
    "#      grupos sub-representados em países específicos podem indicar mercados a serem explorados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desempenho de vendas por grupo*\n",
    "\n",
    "#soma salesytd e saleslastyear**\n",
    "group_performance = sales_salesterritory.groupby('group')[['salesytd', 'saleslastyear']].sum()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "group_performance.plot(kind='bar', color=['blue', 'green'], edgecolor='black', ax=ax)\n",
    "plt.title(\"Desempenho de Vendas por Grupo\", color='white', fontsize=14)\n",
    "plt.xlabel(\"Grupo\", color='white', fontsize=12)\n",
    "plt.ylabel(\"Vendas\", color='white', fontsize=12)\n",
    "plt.xticks(color='white', fontsize=10, rotation=0)\n",
    "ax.yaxis.set_visible(False)\n",
    "plt.legend(['Vendas YTD', 'Vendas Ano Passado'], fontsize=10, loc='upper left')\n",
    "\n",
    "for container in ax.containers:\n",
    "    labels = [f'{value / 1_000_000:.2f}M' for value in container.datavalues]\n",
    "    ax.bar_label(container, labels=labels, label_type='edge', color='white', fontsize=10)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#doc*: comparar o desempenho de vendas entre os diferentes grupos com base nas métricas \"salesytd\" (vendas acumuladas no ano) e \"saleslastyear\" (vendas do ano passado)\n",
    "#      grupos com desempenho consistente indicam estabilidade e podem ser mantidos\n",
    "#      grupos com queda nas vendas requerem atenção para identificar e corrigir problemas\n",
    "\n",
    "\n",
    "#doc: vendas YTD (Year-to-Date): representa o total acumulado de vendas desde o início do ano até a data atual.Útil para avaliar o desempenho atual em relação ao ano anterior\n",
    "#       vendas do Ano Passado (Last Year): refere-se ao total de vendas realizadas durante o ano anterior completo. Comparação para identificar tendências de crescimento ou declínio ao longo do tempo\n",
    "\n",
    "\n",
    "#doc: as colunas 'salesytd' e 'saleslastyear' não são calculadas dinamicamente,elas representam valores acumulados registrados na base de dados até a data \n",
    "#     informada em 'modifieddate', que é 2008-04-30 para todos os registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receita total por país*\n",
    "country_performance = sales_salesterritory.groupby('countryregioncode')['salesytd'].sum().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=country_performance.index, y=country_performance.values, palette='mako')\n",
    "plt.title(\"Receita Total por País\", color='white', fontsize=14)\n",
    "plt.xlabel(\"Código de País\", color='white', fontsize=12)\n",
    "plt.ylabel(\"\", fontsize=12)  \n",
    "plt.xticks(color='white', fontsize=10)\n",
    "ax.tick_params(axis='y', left=False, labelleft=False)\n",
    "\n",
    "for container in ax.containers:\n",
    "    labels = [f'{value / 1_000_000:.2f}M' for value in container.datavalues]\n",
    "    ax.bar_label(container, labels=labels, label_type='edge', color='white', fontsize=10)\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# doc*:representação da receita total (SalesYTD) acumulada por país, destacando quais nações geraram mais receita,\n",
    "#      ajuda a entender a distribuição geográfica das vendas e a identificar mercados mais lucrativos para a empresa\n",
    "\n",
    "# doc: países com maior receita, como os no topo do gráfico, são os principais contribuintes para o faturamento,\n",
    "#      comparação direta entre países auxilia na análise de desempenho regional e na alocação de recursos\n",
    "\n",
    "#doc: a coluna 'salesytd' não é calculada dinamicamente,ela representa valores acumulados registrados na base de dados até a data \n",
    "#     informada em 'modifieddate', que é 2008-04-30 para todos os registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_salesterritory.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis quantitativas*: estatísticas descritivas para verificar se ainda há o que ser feito antes de exportar os dados ao BigQuery\n",
    "\n",
    "# Identificar colunas numéricas para análise de outliers\n",
    "numeric_columns = ['salesytd', 'saleslastyear']\n",
    "\n",
    "# Estatísticas Descritivas das colunas numéricas*\n",
    "print(sales_salesterritory[numeric_columns].describe())\n",
    "\n",
    "# Cálculo de limites para outliers (IQR)**\n",
    "for col in numeric_columns:\n",
    "    q1 = sales_salesterritory[col].quantile(0.25)\n",
    "    q3 = sales_salesterritory[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    # Exibir os limites\n",
    "    print(f\"\\nColuna: {col}\")\n",
    "    print(f\"Limite inferior: {lower_bound}, Limite superior: {upper_bound}\")\n",
    "    \n",
    "    # Detecção e Análise de Outliers***\n",
    "    outliers = sales_salesterritory[(sales_salesterritory[col] < lower_bound) | (sales_salesterritory[col] > upper_bound)]\n",
    "    print(f\"Outliers detectados ({len(outliers)}):\")\n",
    "    print(outliers[[col]])\n",
    "\n",
    "\n",
    "#doc*: variáveis quantitativas são um tipo de dado que pode ser representado por números e medidas objetivas, no nosso caso, vacationhours, sickleavehours, por exemplo\n",
    "#doc*: realizar estatísticas descritivas para entender a centralidade e variação dos dados (valores médios, mínimos, máximos, etc.)\n",
    "#doc**: calcular limites para identificar outliers (valores extremos que podem indicar erros ou casos excepcionais nos dados)\n",
    "#doc***: verificar a existência de outliers para decidir ações como remoção, substituição ou tratamento, garantindo qualidade dos dados\n",
    "#doc****: as colunas analisadas não apresentam outliers, pois os dados estão dentro dos limites esperados, sugerindo que não há necessidade de tratamento adicional para valores extremos,\n",
    "#         isso indica boa qualidade dos dados para essas variáveis e que elas estão prontas para serem exportadas ou utilizadas em análises e modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar o estilo do gráfico para fundo escuro\n",
    "sns.set(style='darkgrid', rc={\"axes.facecolor\": \"black\", \"figure.facecolor\": \"black\"})\n",
    "\n",
    "# Lista das colunas para análise\n",
    "columns_to_plot = ['salesytd', 'saleslastyear']\n",
    "\n",
    "# Dividir os valores por 1.000.000 para representar em milhões\n",
    "sales_salesterritory[columns_to_plot] = sales_salesterritory[columns_to_plot] / 1_000_000\n",
    "\n",
    "# Configurar a grade para 2 gráficos, um embaixo do outro\n",
    "fig, axes = plt.subplots(len(columns_to_plot), 1, figsize=(10, 12))\n",
    "\n",
    "for ax, col in zip(axes, columns_to_plot):\n",
    "    quartiles = sales_salesterritory.groupby('group')[col].quantile([0.25, 0.50, 0.75]).unstack()\n",
    "    sns.boxplot(\n",
    "        x='group', y=col, data=sales_salesterritory, ax=ax, showmeans=True,\n",
    "        meanprops={\"marker\": \"o\", \"markerfacecolor\": \"white\", \"markeredgecolor\": \"black\", \"markersize\": 6},\n",
    "        boxprops={\"color\": \"blue\"}, whiskerprops={\"color\": \"blue\"}, capprops={\"color\": \"blue\"},\n",
    "        medianprops={\"color\": \"red\"}, flierprops={\"markerfacecolor\": \"yellow\", \"markersize\": 6}\n",
    "    )\n",
    "    ax.set_title(f'Distribuição de {col} por Grupo (em milhões)', color='white', fontsize=14)\n",
    "    ax.set_xlabel(\"Grupo\", color='white', fontsize=12)\n",
    "    ax.set_ylabel(\"\", fontsize=12)  \n",
    "    ax.tick_params(axis='x', colors='white', labelrotation=45, labelsize=10)\n",
    "    ax.tick_params(axis='y', colors='white', left=False)  \n",
    "    ax.grid(False)\n",
    "\n",
    "    # Adicionar os quartis como texto\n",
    "    for group, values in quartiles.iterrows():\n",
    "        if values.notnull().all(): \n",
    "            x_pos = list(sales_salesterritory['group'].unique()).index(group)  \n",
    "            offset = 0.42\n",
    "            for i, quartile in enumerate(values):\n",
    "                y_pos = quartile\n",
    "                ax.text(x=x_pos + offset, y=y_pos, s=f\"{quartile:.2f}M\", ha='left', va='center', fontsize=10, color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualizar o dicionário df_processados com o df ajustado\n",
    "df_processados['sales_salesterritory'] = sales_salesterritory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_salesterritory.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padronizar colunas com valores textuais\n",
    "sales_salesterritory['name'] = sales_salesterritory['name'].str.strip().str.upper()\n",
    "sales_salesterritory['countryregioncode'] = sales_salesterritory['countryregioncode'].str.strip().str.upper()\n",
    "sales_salesterritory['group'] = sales_salesterritory['group'].str.strip().str.upper()\n",
    "sales_salesterritory['rowguid'] = sales_salesterritory['rowguid'].str.strip().str.upper()\n",
    "\n",
    "print(sales_salesterritory.head())\n",
    "\n",
    "#doc: padronizar as strings nessa etapa, contribui para a execução das demais etapas do pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Garantir que apenas tabelas únicas sejam exportadas\n",
    "unique_df_processados = {k: v for k, v in df_processados.items()}\n",
    "\n",
    "# Exportar tabelas para o BigQuery\n",
    "for table_name, df_cleaned in unique_df_processados.items():\n",
    "    # Nome da tabela no BigQuery\n",
    "    output_table = f\"{output_dataset}.{table_name}\"\n",
    "\n",
    "    # Configurar job de exportação\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_TRUNCATE\"  \n",
    "    )\n",
    "    \n",
    "    # Exportar DataFrame para o BigQuery\n",
    "    job = client.load_table_from_dataframe(df_cleaned, output_table, job_config=job_config)\n",
    "    job.result()\n",
    "\n",
    "    print(f\"Tabela {table_name} exportada com sucesso para {output_table}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
