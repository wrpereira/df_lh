{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP INICIAL DO PROJETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#importação das bibliotecase e pacotes necessários para a análise\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas_gbq as gbq\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery_storage import BigQueryReadClient\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "\n",
    "# Carrega o .env: onde estão as credenciais do projeto/repositório\n",
    "load_dotenv(\"/mnt/c/Users/wrpen/OneDrive/Desktop/df_lh/.env\")\n",
    "\n",
    "# Detectar ambiente: como eu estou usando wsl-ubuntu, no VS Code  -  Windows, estava dando conflitos de path\n",
    "if os.name == \"nt\":  # se Windows\n",
    "    credentials_path = r\"C:\\Temp\\desafiolh-445818-3cb0f62cb9ef.json\"\n",
    "else:  # se WSL/Linux\n",
    "    credentials_path = \"/mnt/c/Temp/desafiolh-445818-3cb0f62cb9ef.json\"\n",
    "\n",
    "# Parâmetros injetados pelo Papermill ou definidos manualmente, caso não existam no ambiente\n",
    "# Tables_to_process: lista de tabelas que serão processadas\n",
    "# Output_dataset: nome do dataset onde os dados processados serão armazenados, neste caso, raw_data_cleaned\n",
    "if 'tables_to_process' not in locals():\n",
    "    tables_to_process = [\n",
    "        \"desafioadventureworks-446600.stg_marts_tables.dim_product_forecast\"  ,\n",
    "        \"desafioadventureworks-446600.stg_marts_tables.dim_product_seasonality\",\n",
    "        \"desafioadventureworks-446600.stg_staging_tables.stg_production_product\",\n",
    "        \"desafioadventureworks-446600.stg_staging_tables.stg_sales_store\"\n",
    "\n",
    "    ]\n",
    "\n",
    "if 'output_dataset' not in locals():\n",
    "    output_dataset = \"desafioadventureworks-446600.stg_marts_tables\"\n",
    "\n",
    "# Configs do cliente BigQuery: input de project e location de acordo com dados no Bigquery\n",
    "credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "client = bigquery.Client(credentials=credentials, project=os.getenv(\"BIGQUERY_PROJECT\"), location=\"us-central1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabelas a processar: ['desafioadventureworks-446600.stg_marts_tables.dim_product_forecast', 'desafioadventureworks-446600.stg_marts_tables.dim_product_seasonality', 'desafioadventureworks-446600.stg_staging_tables.stg_production_product', 'desafioadventureworks-446600.stg_staging_tables.stg_sales_store']\n"
     ]
    }
   ],
   "source": [
    "# Print com a tabela que vai ser processada nesse notebook\n",
    "\n",
    "print(\"Tabelas a processar:\", tables_to_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) e Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glossário dos dados:\n",
    "\n",
    "O termo ''doc:'', situado no rodapé de algumas cells, indica algo como:\n",
    "\n",
    "- documentação: documentar decisões, análises e resultados;\n",
    "\n",
    "- abreviações de termos, como bkp, df, entre outros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração para que o df exiba todas as colunas e todas as linhas completas, e também, exiba o formato numérico com 2 dígitos após a vírgula\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 10000)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "\n",
    "#doc: df = dataframe  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando tabela: desafioadventureworks-446600.stg_marts_tables.dim_product_forecast\n",
      "Lendo os dados do BigQuery...\n",
      "Tabela dim_product_forecast processada e armazenada com sucesso.\n",
      "Processando tabela: desafioadventureworks-446600.stg_marts_tables.dim_product_seasonality\n",
      "Lendo os dados do BigQuery...\n",
      "Tabela dim_product_seasonality processada e armazenada com sucesso.\n",
      "Processando tabela: desafioadventureworks-446600.stg_staging_tables.stg_production_product\n",
      "Lendo os dados do BigQuery...\n",
      "Tabela stg_production_product processada e armazenada com sucesso.\n",
      "Processando tabela: desafioadventureworks-446600.stg_staging_tables.stg_sales_store\n",
      "Lendo os dados do BigQuery...\n",
      "Tabela stg_sales_store processada e armazenada com sucesso.\n",
      "Todas as tabelas foram processadas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Dicionário para armazenar os df processados\n",
    "df_processados = {}\n",
    "\n",
    "# Iteração das tabelas e armazenamento em df\n",
    "for input_table in tables_to_process:\n",
    "    print(f\"Processando tabela: {input_table}\")\n",
    "    \n",
    "    # Nome da tabela com substituição de '-' por '_'\n",
    "    table_name = input_table.split(\".\")[-1].replace(\"-\", \"_\")  \n",
    "    \n",
    "    # Ler os dados da tabela do BigQuery para um df\n",
    "    print(\"Lendo os dados do BigQuery...\")\n",
    "    query = f\"SELECT * FROM `{input_table}`\"\n",
    "    table_data = client.query(query).to_dataframe()\n",
    "    \n",
    "    # Armazenar o df no dicionário\n",
    "    df_processados[table_name] = table_data\n",
    "    print(f\"Tabela {table_name} processada e armazenada com sucesso.\")\n",
    "\n",
    "# Print de validação\n",
    "print(\"Todas as tabelas foram processadas com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variável criada: dim_product_forecast\n",
      "Variável criada: dim_product_seasonality\n",
      "Variável criada: stg_production_product\n",
      "Variável criada: stg_sales_store\n"
     ]
    }
   ],
   "source": [
    "# Listar todas as variáveis criadas dinamicamente\n",
    "for table_name in df_processados.keys():\n",
    "    print(f\"Variável criada: {table_name}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'businessentityid_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16144\\3360497124.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'productid_id'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Realizar merge com stg_sales_store usando 'businessentityid_id' como chave\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m consolidated_df = consolidated_df.merge(\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mdf_processados\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stg_sales_store'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'businessentityid_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'store_nm'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mleft_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'businessentityid_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\wrpen\\OneDrive\\Desktop\\df_lh\\.venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10483\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMergeValidate\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10484\u001b[0m     ) -> DataFrame:\n\u001b[0;32m  10485\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10487\u001b[1;33m         return merge(\n\u001b[0m\u001b[0;32m  10488\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10489\u001b[0m             \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10490\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\wrpen\\OneDrive\\Desktop\\df_lh\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         )\n\u001b[0;32m    168\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         op = _MergeOperation(\n\u001b[0m\u001b[0;32m    170\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\wrpen\\OneDrive\\Desktop\\df_lh\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\wrpen\\OneDrive\\Desktop\\df_lh\\.venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1287\u001b[1;33m                         \u001b[0mleft_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1288\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1289\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m                         \u001b[1;31m# work-around for merge_asof(left_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\wrpen\\OneDrive\\Desktop\\df_lh\\.venv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1840\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1844\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'businessentityid_id'"
     ]
    }
   ],
   "source": [
    "# Realizar merge entre dim_product_forecast e stg_production_product para incluir o nome do produto\n",
    "consolidated_df = df_processados['dim_product_forecast'].merge(\n",
    "    df_processados['stg_production_product'][['productid_id', 'product_nm']],\n",
    "    how='left',\n",
    "    on='productid_id'\n",
    ")\n",
    "\n",
    "# Realizar merge com stg_sales_store usando 'businessentityid_id' como chave\n",
    "consolidated_df = consolidated_df.merge(\n",
    "    df_processados['stg_sales_store'][['businessentityid_id', 'store_nm']],\n",
    "    how='left',\n",
    "    left_on='businessentityid_id',\n",
    "    right_on='businessentityid_id'\n",
    ")\n",
    "\n",
    "# Se necessário, realizar merge com outras tabelas (exemplo: sazonalidade)\n",
    "consolidated_df = consolidated_df.merge(\n",
    "    df_processados['dim_product_seasonality'],\n",
    "    how='left',\n",
    "    on=['productid_id', 'businessentityid_id']\n",
    ")\n",
    "\n",
    "# Visualizar as primeiras linhas para verificar a consolidação\n",
    "print(consolidated_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atribuir o df a uma variável com nome mais simples\n",
    "dim_product_forecast = df_processados['dim_product_forecast']\n",
    "\n",
    "print(f\"Colunas: {dim_product_forecast.shape[1]}\\nLinhas: {dim_product_forecast.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar e exibir o df por 'productid_id'\n",
    "dim_product_forecast = dim_product_forecast.sort_values(by=['productid_id'])\n",
    "\n",
    "print(dim_product_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar por todas as colunas do df, para verificar valores ausentes\n",
    "\n",
    "# Verificar valores ausentes na coluna\n",
    "for column in dim_product_forecast.columns:   \n",
    "    missing_rows = dim_product_forecast[dim_product_forecast[column].isnull()]\n",
    "    print(f\"Coluna '{column}': {missing_rows.shape[0]} linhas ausentes.\")\n",
    "    \n",
    "# Mostrar as primeiras linhas ausentes, se preciso for, limitar o head() para dar menos outputs ou limitar os outputs\n",
    "    if not missing_rows.empty:\n",
    "        print(f\"Exibindo as primeiras linhas com valores ausentes em '{column}':\")\n",
    "        print(missing_rows.head(), \"\\n\")\n",
    "    else:\n",
    "        print(f\"Nenhuma linha com valores ausentes em '{column}'.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores únicos por coluna, para verificar se colunas como flags, normalmente booleanas, possuem apenas 1 ou 2 valores.\n",
    "\n",
    "valores_unicos = dim_product_forecast.nunique(dropna=False)\n",
    "\n",
    "print(\"Valores únicos incluindo NaN:\")\n",
    "print(valores_unicos)\n",
    "\n",
    "#doc: currentflag possue somente 1 valor, o que indica que pode ser somente valores True ou False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificar informações do df\n",
    "dim_product_forecast.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visão Geral da Previsão: \n",
    "\n",
    "# Total de demanda por loja*\n",
    "\n",
    "demand_by_store = dim_product_forecast.groupby('store_id')['forecast_quantity'].sum().reset_index()\n",
    "demand_by_store = demand_by_store.sort_values(by='forecast_quantity', ascending=False)\n",
    "\n",
    "print(demand_by_store.head(15))\n",
    "\n",
    "\n",
    "#doc*: soma da previsão de demanda agrupada por loja\n",
    "\n",
    "\n",
    "# Conclusões sobre a previsão de demanda por loja:\n",
    "\n",
    "# 1. A loja com o ID 4 possui a maior previsão de demanda (17,850,928.00), indicando ser a principal \n",
    "#    consumidora de produtos nos próximos 3 meses. Isso sugere que ela deve receber maior atenção\n",
    "#    na distribuição de produtos e planejamento de estoque.\n",
    "\n",
    "# 2. A loja com o ID 6 é a segunda maior em previsão de demanda (14,796,192.00), também se destacando\n",
    "#    como um centro importante para alocação de produtos.\n",
    "\n",
    "# 3. As lojas com IDs 1, 10 e 2 completam o top 5, apresentando demandas significativamente menores,\n",
    "#    mas ainda consideráveis. Estas lojas podem ser consideradas secundárias em termos de prioridade.\n",
    "\n",
    "# 4. A diferença entre as lojas com maior (ID 4) e menor (ID 8) demanda no top 10 é de aproximadamente\n",
    "#    15,184,704 unidades. Isso mostra uma concentração de demanda em certas lojas.\n",
    "\n",
    "# 5. O padrão de demanda evidencia que os esforços logísticos e de distribuição devem ser direcionados\n",
    "#    para lojas de alta demanda (principalmente IDs 4 e 6), enquanto as demais podem ser monitoradas\n",
    "#    com menos frequência.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico ajustado de barras para lojas sem valores no eixo Y e com valores no topo das barras\n",
    "\n",
    "sns.set(style=\"white\")  # Fundo branco\n",
    "plt.figure(figsize=(10, 6))  # Ajustar o tamanho do gráfico\n",
    "\n",
    "sns.barplot(data=demand_by_store, x='store_id', y='forecast_quantity', palette='Blues_d')\n",
    "plt.title('Demanda Prevista por Loja')\n",
    "plt.xlabel('Loja (Store ID)')\n",
    "plt.ylabel('')  # Remover o rótulo do eixo Y\n",
    "\n",
    "# Remover valores do eixo Y\n",
    "plt.gca().yaxis.set_ticks([])\n",
    "\n",
    "# Adicionar valores no topo das barras com uma casa decimal\n",
    "for index, row in demand_by_store.iterrows():\n",
    "    plt.text(\n",
    "        x=index, \n",
    "        y=row['forecast_quantity'], \n",
    "        s=f\"{row['forecast_quantity']/1000000:.1f}M\", \n",
    "        ha='center', \n",
    "        va='bottom', \n",
    "        fontsize=10, \n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total de demanda por produto*:\n",
    "\n",
    "demand_by_product = dim_product_forecast.groupby('productid_id')['forecast_quantity'].sum().reset_index()\n",
    "demand_by_product = demand_by_product.sort_values(by='forecast_quantity', ascending=False)\n",
    "\n",
    "print(demand_by_product.head(10))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Conclusões sobre a previsão de demanda por produto:\n",
    "\n",
    "# 1. O produto com ID 712 possui a maior previsão de demanda (3,996,816.00), indicando que ele será o mais requisitado\n",
    "#    entre todos os produtos nos próximos 3 meses. Este produto deve ser priorizado na produção e reposição.\n",
    "\n",
    "# 2. O produto com ID 715 é o segundo em previsão de demanda (3,291,088.00), seguido pelos IDs 711 (3,199,136.00),\n",
    "#    708 (3,066,240.00) e 707 (2,890,224.00). Esses produtos compõem o top 5 em demanda prevista.\n",
    "\n",
    "# 3. A diferença entre o produto mais demandado (ID 712) e o quinto mais demandado (ID 707) é de aproximadamente\n",
    "#    1,106,592.00 unidades, indicando uma concentração de demanda significativa nos produtos mais procurados.\n",
    "\n",
    "# 4. A presença de produtos com alta demanda relativamente próxima indica que esses itens precisam ser monitorados\n",
    "#    como um grupo prioritário para ajustes em produção e estoque, dependendo das vendas reais.\n",
    "\n",
    "# 5. Produtos fora do top 5, como o ID 714 (1,729,872.00), ainda apresentam volumes consideráveis de demanda,\n",
    "#    mas podem ser considerados de prioridade secundária em um cenário de otimização de recursos.\n",
    "\n",
    "#doc: os dados são baseados em previsões de demanda nos próximos 3 meses, agrupados por produto.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar estilo do gráfico\n",
    "sns.set(style=\"white\")  # Fundo branco\n",
    "plt.figure(figsize=(10, 6))  # Ajustar o tamanho do gráfico\n",
    "\n",
    "# Selecionar apenas os Top 10 produtos com maior demanda\n",
    "top_products = demand_by_product.sort_values(by='forecast_quantity', ascending=False).head(10)\n",
    "\n",
    "# Gráfico de barras verticais para produtos com ordenação\n",
    "ax = sns.barplot(\n",
    "    data=top_products, \n",
    "    x='productid_id', \n",
    "    y='forecast_quantity', \n",
    "    palette='Blues_d',\n",
    "    order=top_products['productid_id']\n",
    ")\n",
    "\n",
    "plt.title('Top 10 Produtos com Maior Demanda Prevista')\n",
    "plt.xlabel('Produto (Product ID)')\n",
    "plt.ylabel('')  # Remover o rótulo do eixo Y\n",
    "\n",
    "# Remover valores do eixo Y\n",
    "plt.gca().yaxis.set_ticks([])\n",
    "\n",
    "# Adicionar valores no topo das barras com uma casa decimal\n",
    "for bar in ax.patches:\n",
    "    ax.text(\n",
    "        x=bar.get_x() + bar.get_width() / 2, \n",
    "        y=bar.get_height(), \n",
    "        s=f\"{bar.get_height() / 1000000:.3f}M\", \n",
    "        ha='center', \n",
    "        va='bottom', \n",
    "        fontsize=10, \n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sazonalidade - Produto ID == 712"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atribuir o df a uma variável com nome mais simples\n",
    "dim_product_seasonality = df_processados['dim_product_seasonality']\n",
    "\n",
    "print(f\"Colunas: {dim_product_seasonality.shape[1]}\\nLinhas: {dim_product_seasonality.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_product_seasonality.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar os dados para o produto 712\n",
    "filtered_product = dim_product_seasonality[dim_product_seasonality['productid_id'] == 712]\n",
    "\n",
    "# Criar uma coluna 'sale_month' para exibição no gráfico\n",
    "filtered_product['sale_month'] = filtered_product['year'].astype(str) + '-' + filtered_product['month'].astype(str).str.zfill(2)\n",
    "\n",
    "# Agrupar por 'sale_month' e somar a quantidade total vendida\n",
    "monthly_sales = filtered_product.groupby('sale_month')['total_quantity_sold'].sum().reset_index()\n",
    "\n",
    "# Ordenar os meses para garantir a sequência correta\n",
    "monthly_sales = monthly_sales.sort_values(by='sale_month')\n",
    "\n",
    "# Visualizar os dados em um gráfico de linhas com valores nos marcadores\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=monthly_sales, x='sale_month', y='total_quantity_sold', marker='o')\n",
    "\n",
    "plt.title('Variação Mensal de Vendas - Produto 712')\n",
    "plt.xlabel('Mês (Ano-Mês)')\n",
    "plt.ylabel('Quantidade Vendida')\n",
    "plt.xticks(rotation=45)  # Rotacionar os rótulos para melhor leitura\n",
    "plt.grid(visible=True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Adicionar os valores nos marcadores\n",
    "for i, row in monthly_sales.iterrows():\n",
    "    plt.text(\n",
    "        x=row['sale_month'], \n",
    "        y=row['total_quantity_sold'] + 5,  # Ajustar a posição acima do marcador\n",
    "        s=row['total_quantity_sold'], \n",
    "        ha='center', \n",
    "        fontsize=9, \n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atribuir o df a uma variável com nome mais simples\n",
    "stg_production_product = df_processados['stg_production_product']\n",
    "\n",
    "print(f\"Colunas: {stg_production_product.shape[1]}\\nLinhas: {stg_production_product.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer o merge para adicionar o nome do produto\n",
    "dim_product_seasonality = dim_product_seasonality.merge(\n",
    "    stg_production_product, \n",
    "    how='left', \n",
    "    on='productid_id'\n",
    ")\n",
    "\n",
    "# Garantir que o produto com ID 712 está correto\n",
    "product_name = dim_product_seasonality[dim_product_seasonality['productid_id'] == 712]['product_nm'].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar os dados para o produto 712\n",
    "filtered_product = dim_product_seasonality[dim_product_seasonality['productid_id'] == 712]\n",
    "\n",
    "# Obter o nome do produto (garantir que ele existe no merge)\n",
    "product_name = filtered_product['product_nm'].iloc[0] if not filtered_product.empty else \"Desconhecido\"\n",
    "\n",
    "# Criar uma coluna 'sale_month' para exibição no gráfico\n",
    "filtered_product['sale_month'] = filtered_product['year'].astype(str) + '-' + filtered_product['month'].astype(str).str.zfill(2)\n",
    "\n",
    "# Agrupar por 'sale_month' e somar a quantidade total vendida\n",
    "monthly_sales = filtered_product.groupby('sale_month')['total_quantity_sold'].sum().reset_index()\n",
    "\n",
    "# Ordenar os meses para garantir a sequência correta\n",
    "monthly_sales = monthly_sales.sort_values(by='sale_month')\n",
    "\n",
    "# Visualizar os dados em um gráfico de linhas com valores nos marcadores\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=monthly_sales, x='sale_month', y='total_quantity_sold', marker='o')\n",
    "\n",
    "plt.title(f'Variação Mensal de Vendas - {product_name} (ID: 712)')\n",
    "plt.xlabel('Mês (Ano-Mês)')\n",
    "plt.ylabel('Quantidade Vendida')\n",
    "plt.xticks(rotation=45)  # Rotacionar os rótulos para melhor leitura\n",
    "plt.grid(visible=True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Adicionar os valores nos marcadores\n",
    "for i, row in monthly_sales.iterrows():\n",
    "    plt.text(\n",
    "        x=row['sale_month'], \n",
    "        y=row['total_quantity_sold'] + 5,  # Ajustar a posição acima do marcador\n",
    "        s=row['total_quantity_sold'], \n",
    "        ha='center', \n",
    "        fontsize=9, \n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualizar o dicionário df_processados com o df ajustado\n",
    "df_processados['dim_product_forecast'] = dim_product_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padronizar colunas com valores textuais\n",
    "dim_product_forecast['jobtitle'] = dim_product_forecast['jobtitle'].str.strip().str.upper()\n",
    "dim_product_forecast['gender'] = dim_product_forecast['gender'].str.strip().str.upper()\n",
    "dim_product_forecast['maritalstatus'] = dim_product_forecast['maritalstatus'].str.strip().str.upper()\n",
    "dim_product_forecast['loginid'] = dim_product_forecast['loginid'].str.strip().str.upper()\n",
    "dim_product_forecast['rowguid'] = dim_product_forecast['rowguid'].str.strip().str.upper()\n",
    "\n",
    "print(dim_product_forecast.head())\n",
    "\n",
    "#doc: padronizar as strings nessa etapa, contribui para a execução das demais etapas do pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Garantir que apenas tabelas únicas sejam exportadas\n",
    "unique_df_processados = {k: v for k, v in df_processados.items()}\n",
    "\n",
    "# Exportar tabelas para o BigQuery\n",
    "for table_name, df_cleaned in unique_df_processados.items():\n",
    "    # Nome da tabela no BigQuery\n",
    "    output_table = f\"{output_dataset}.{table_name}\"\n",
    "\n",
    "    # Configurar job de exportação\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_TRUNCATE\"  \n",
    "    )\n",
    "    \n",
    "    # Exportar DataFrame para o BigQuery\n",
    "    job = client.load_table_from_dataframe(df_cleaned, output_table, job_config=job_config)\n",
    "    job.result()\n",
    "\n",
    "    print(f\"Tabela {table_name} exportada com sucesso para {output_table}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
