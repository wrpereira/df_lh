{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA (Exploratory Data Analysis) & Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##import das bibliotecas e adequando colunas, linhas e formato de números\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import pandas_gbq as gbq\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.bigquery_storage import BigQueryReadClient\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Carrega o .env\n",
    "load_dotenv(\"/mnt/c/Users/wrpen/OneDrive/Desktop/df_lh/.env\")\n",
    "\n",
    "# Detectar ambiente\n",
    "if os.name == \"nt\":  # Windows\n",
    "    credentials_path = r\"C:\\Temp\\desafiolh-445818-3cb0f62cb9ef.json\"\n",
    "else:  # WSL/Linux\n",
    "    credentials_path = \"/mnt/c/Temp/desafiolh-445818-3cb0f62cb9ef.json\"\n",
    "\n",
    "\n",
    "# Parâmetros injetados pelo Papermill ou definidos manualmente\n",
    "if 'tables_to_process' not in locals():\n",
    "    tables_to_process = [\n",
    "        \"desafioadventureworks-446600.raw_data.humanresources_employee\"       \n",
    "    ]\n",
    "\n",
    "if 'output_dataset' not in locals():\n",
    "    output_dataset = \"desafioadventureworks-446600.raw_data_cleaned\"\n",
    "\n",
    "\n",
    "# Configurar o cliente do BigQuery com project e location dinâmicos\n",
    "credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "client = bigquery.Client(credentials=credentials, project=os.getenv(\"BIGQUERY_PROJECT\"), location=\"us-central1\")\n",
    "\n",
    "\n",
    "# Verificar se a configuração está correta\n",
    "print(\"Credenciais do BigQuery:\", os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"))\n",
    "\n",
    "# Verifica se a variável está configurada\n",
    "print(os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tabelas a processar:\", tables_to_process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "client = bigquery.Client(credentials=credentials, project=\"desafioadventureworks-446600\", location=\"us-central1\")\n",
    "\n",
    "\n",
    "# # Configurar o cliente do BigQuery\n",
    "# client = bigquery.Client()\n",
    "\n",
    "# Nome do dataset e tabela\n",
    "dataset_id = 'raw_data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar Pandas para exibir todas as colunas e todas as linhas completas\n",
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar tabelas no dataset\n",
    "tables = client.list_tables('raw_data')\n",
    "print(\"Tabelas disponíveis:\")\n",
    "for table in tables:\n",
    "    print(table.table_id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o cliente do BigQuery\n",
    "client = bigquery.Client(credentials=credentials, project=\"desafioadventureworks-446600\", location=\"us-central1\")\n",
    "\n",
    "# Configurar o cliente do BigQuery com project e location dinâmicos\n",
    "credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "\n",
    "# Inicializar o cliente do BigQuery Storage\n",
    "bqstorage_client = BigQueryReadClient(credentials=credentials)\n",
    "\n",
    "\n",
    "# Dicionário para armazenar DataFrames processados\n",
    "processed_data = {}\n",
    "\n",
    "# Processar tabelas e armazenar DataFrames\n",
    "for input_table in tables_to_process:\n",
    "    print(f\"Processando tabela: {input_table}\")\n",
    "    \n",
    "    # Nome da tabela\n",
    "    table_name = input_table.split(\".\")[-1]  # Extrai o nome da tabela\n",
    "    \n",
    "    # Etapa 1: Ler os dados da tabela do BigQuery com pyarrow\n",
    "    print(\"Lendo os dados do BigQuery...\")\n",
    "    query = f\"SELECT * FROM `{input_table}`\"\n",
    "    EDA_humanresources_employee_raw = client.query(query).to_dataframe(bqstorage_client=bqstorage_client)\n",
    "\n",
    "    # Etapa 2: Transformar JSON em formato tabular\n",
    "    print(\"Transformando os dados para formato tabular...\")\n",
    "\n",
    "    # Verificar se há colunas com dados em formato JSON\n",
    "    if EDA_humanresources_employee_raw.shape[1] == 1 and isinstance(EDA_humanresources_employee_raw.iloc[0, 0], str):\n",
    "        try:\n",
    "            print(\"Normalizando dados JSON...\")\n",
    "            # Substituir `null` por `None` e carregar o JSON\n",
    "            EDA_humanresources_employee = pd.json_normalize(\n",
    "                EDA_humanresources_employee_raw.iloc[:, 0].apply(lambda x: json.loads(x.replace(\"null\", \"None\")))\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao normalizar JSON: {e}\")\n",
    "            EDA_humanresources_employee = EDA_humanresources_employee  # Caso falhe, mantém os dados brutos\n",
    "    else:\n",
    "        EDA_humanresources_employee = EDA_humanresources_employee_raw\n",
    "\n",
    "    # Armazenar o DataFrame limpo em um dicionário\n",
    "    processed_data[table_name] = EDA_humanresources_employee\n",
    "    print(f\"Tabela {table_name} processada e armazenada com sucesso.\")\n",
    "\n",
    "# Após o loop, exibir uma mensagem de conclusão\n",
    "print(\"Todas as tabelas foram processadas com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(EDA_humanresources_employee_raw.iloc[:, 0].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_and_load_json(value):\n",
    "#     \"\"\"Função para corrigir e carregar JSON.\"\"\"\n",
    "#     try:\n",
    "#         # Substituir `null` por `None` e carregar o JSON\n",
    "#         value = value.replace(\"null\", \"null\")\n",
    "#         return json.loads(value)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erro ao processar JSON: {e}, valor problemático: {value}\")\n",
    "#         return None  # Retorna None se o valor for inválido\n",
    "\n",
    "# # Normalizar os dados JSON\n",
    "# print(\"Normalizando os dados JSON...\")\n",
    "# try:\n",
    "#     EDA_humanresources_employee = pd.json_normalize(\n",
    "#         EDA_humanresources_employee_raw.iloc[:, 0].apply(clean_and_load_json)\n",
    "#     )\n",
    "#     print(\"Dados normalizados com sucesso!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Erro ao normalizar os dados JSON: {e}\")\n",
    "#     EDA_humanresources_employee = EDA_humanresources_employee_raw  # Mantém os dados originais em caso de erro\n",
    "\n",
    "# print(f\"Tabela processada: {input_table}\")\n",
    "# print(EDA_humanresources_employee.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_and_load_json(value):\n",
    "    \"\"\"Função para corrigir e carregar JSON.\"\"\"\n",
    "    try:\n",
    "        # Substituir `null` por `None` e carregar o JSON\n",
    "        value = value.replace(\"null\", \"None\")\n",
    "        return json.loads(value)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar JSON: {e}, valor problemático: {value}\")\n",
    "        return None  # Retorna None se o valor for inválido\n",
    "\n",
    "# Normalizar os dados JSON\n",
    "print(\"Normalizando os dados JSON...\")\n",
    "try:\n",
    "    EDA_humanresources_employee = pd.json_normalize(\n",
    "        EDA_humanresources_employee_raw.iloc[:, 0].apply(clean_and_load_json)\n",
    "    )\n",
    "    print(\"Dados normalizados com sucesso!\")\n",
    "\n",
    "    # Atribuir tipos às colunas\n",
    "    EDA_humanresources_employee['businessentityid'] = EDA_humanresources_employee['businessentityid'].astype('int64', errors='ignore')\n",
    "    EDA_humanresources_employee['nationalidnumber'] = EDA_humanresources_employee['nationalidnumber'].astype('int64', errors='ignore')\n",
    "    EDA_humanresources_employee['loginid'] = EDA_humanresources_employee['loginid'].astype('int64', errors='ignore')\n",
    "    EDA_humanresources_employee['jobtitle'] = EDA_humanresources_employee['jobtitle'].astype('str', errors='ignore')\n",
    "    EDA_humanresources_employee['birthdate'] = pd.to_datetime(EDA_humanresources_employee['birthdate'], errors='coerce')\n",
    "    EDA_humanresources_employee['maritalstatus'] = EDA_humanresources_employee['maritalstatus'].astype('str', errors='ignore')\n",
    "    EDA_humanresources_employee['gender'] = EDA_humanresources_employee['gender'].astype('str', errors='ignore')\n",
    "    EDA_humanresources_employee['hiredate'] = pd.to_datetime(EDA_humanresources_employee['hiredate'], errors='coerce')\n",
    "    EDA_humanresources_employee['salariedflag'] = EDA_humanresources_employee['salariedflag'].astype('bool')\n",
    "    EDA_humanresources_employee['vacationhours'] = EDA_humanresources_employee['vacationhours'].astype('int64', errors='ignore')\n",
    "    EDA_humanresources_employee['sickleavehours'] = EDA_humanresources_employee['sickleavehours'].astype('int64', errors='ignore')\n",
    "    EDA_humanresources_employee['currentflag'] = EDA_humanresources_employee['currentflag'].astype('bool')\n",
    "    EDA_humanresources_employee['rowguid'] = EDA_humanresources_employee['rowguid'].astype('str', errors='ignore')\n",
    "    EDA_humanresources_employee['modifieddate'] = pd.to_datetime(EDA_humanresources_employee['modifieddate'], errors='coerce')\n",
    "    EDA_humanresources_employee['organizationnode'] = EDA_humanresources_employee['organizationnode'].astype('str', errors='ignore')\n",
    "\n",
    "    print(\"Tipos atribuídos com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao normalizar os dados JSON: {e}\")\n",
    "    EDA_humanresources_employee = EDA_humanresources_employee_raw  # Mantém os dados originais em caso de erro\n",
    "\n",
    "print(f\"Tabela processada: {input_table}\")\n",
    "\n",
    "print(EDA_humanresources_employee.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensões do df antes de remover duplicatas\n",
    "\n",
    "EDA_humanresources_employee.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Colunas disponíveis no DataFrame limpo (cleaned):\", EDA_humanresources_employee.columns)\n",
    "\n",
    "# Identificar duplicatas com base em 'businessentityid'\n",
    "duplicatas = EDA_humanresources_employee[\n",
    "    EDA_humanresources_employee.duplicated(subset=['businessentityid'], keep=False)\n",
    "]\n",
    "\n",
    "# Verificar se existem duplicatas\n",
    "if not duplicatas.empty:\n",
    "    # Ordenar duplicatas por 'businessentityid' e 'modifieddate'\n",
    "    duplicatas_ordenadas = duplicatas.sort_values(by=['businessentityid', 'modifieddate'])\n",
    "\n",
    "    # Exibir duplicatas ordenadas\n",
    "    print(\"Duplicatas ordenadas:\")\n",
    "    print(duplicatas_ordenadas)\n",
    "else:\n",
    "    print(\"Não foram encontradas duplicatas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover duplicatas mantendo a última ocorrência com base em 'modifieddate'\n",
    "EDA_humanresources_employee = EDA_humanresources_employee.drop_duplicates(subset=['businessentityid'], keep='last')\n",
    "\n",
    "print(f\"Linhas após remover duplicatas (baseando-se na última 'modifieddate'): {len(EDA_humanresources_employee)}\")\n",
    "\n",
    "#cópia dados brutos\n",
    "raw_data_bkp_2_sem_duplicatas = EDA_humanresources_employee.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar o DataFrame por 'businessentityid' e 'modifieddate'\n",
    "EDA_humanresources_employee = EDA_humanresources_employee.sort_values(by=['businessentityid', 'modifieddate'])\n",
    "\n",
    "print(EDA_humanresources_employee)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Certifique-se de que as colunas de datas está sendo reconhecida corretamente como contendo valores nulos (NaN em pandas). (Não pode object)\n",
    "\n",
    "print(EDA_humanresources_employee.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar as colunas de data\n",
    "date_columns = ['birthdate', 'hiredate', 'modifieddate']\n",
    "\n",
    "# Converter todas as colunas para datetime\n",
    "for col in date_columns:\n",
    "    EDA_humanresources_employee[col] = pd.to_datetime(\n",
    "        EDA_humanresources_employee[col], errors='coerce'\n",
    "    )\n",
    "\n",
    "# Criar uma cópia do DataFrame para exportação no formato JSON\n",
    "datas_formatadas = EDA_humanresources_employee.copy()\n",
    "\n",
    "# Formatar colunas no formato ISO 8601 para BigQuery e tratar nulos como null\n",
    "for col in date_columns:\n",
    "    datas_formatadas[col] = datas_formatadas[col].apply(\n",
    "        lambda x: x.isoformat() if pd.notnull(x) else None  # Certifique-se de que é datetime\n",
    "    )\n",
    "\n",
    "print(EDA_humanresources_employee.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar por todas as colunas do DataFrame\n",
    "\n",
    "for column in EDA_humanresources_employee.columns:\n",
    "    # Verificar valores ausentes na coluna\n",
    "    missing_rows = EDA_humanresources_employee[EDA_humanresources_employee[column].isnull()]\n",
    "    print(f\"Coluna '{column}': {missing_rows.shape[0]} linhas ausentes.\")\n",
    "    \n",
    "    # Mostrar as primeiras linhas ausentes (limitar para não poluir a saída)\n",
    "    if not missing_rows.empty:\n",
    "        print(f\"Exibindo as primeiras linhas com valores ausentes em '{column}':\")\n",
    "        print(missing_rows.head(), \"\\n\")\n",
    "    else:\n",
    "        print(f\"Nenhuma linha com valores ausentes em '{column}'.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preencher 'modifieddate' ausente ou igual a 'hiredate', pois pode ser a ultima data de modificação no sistema.\n",
    "EDA_humanresources_employee.loc[EDA_humanresources_employee['modifieddate'].isnull() | (EDA_humanresources_employee['modifieddate'] == pd.Timestamp('1900-01-01')), 'modifieddate'] = EDA_humanresources_employee['hiredate']\n",
    "\n",
    "# Exibir as linhas ajustadas\n",
    "print(\"Linhas onde 'modifieddate' foi ajustado para 'hiredate':\")\n",
    "print(EDA_humanresources_employee.loc[EDA_humanresources_employee['modifieddate'] == EDA_humanresources_employee['hiredate']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma cópia do DataFrame para exportação no formato JSON\n",
    "ajustes_date_time = EDA_humanresources_employee.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valores únicos por coluna\n",
    "\n",
    "valores_unicos = EDA_humanresources_employee.nunique(dropna=False)\n",
    "\n",
    "print(\"Valores únicos incluindo NaN:\")\n",
    "print(valores_unicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropar colunas vazias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padronizar textos em title ou upper\n",
    "EDA_humanresources_employee['jobtitle'] = EDA_humanresources_employee['jobtitle'].str.strip().str.title()\n",
    "EDA_humanresources_employee['gender'] = EDA_humanresources_employee['gender'].str.strip().str.upper()\n",
    "EDA_humanresources_employee['maritalstatus'] = EDA_humanresources_employee['maritalstatus'].str.strip().str.upper()\n",
    "\n",
    "\n",
    "# Verificar valores únicos para garantir a padronização\n",
    "print(\"Valores únicos em 'jobtitle':\", EDA_humanresources_employee['jobtitle'].unique())\n",
    "print(\"Valores únicos em 'gender':\", EDA_humanresources_employee['gender'].unique())\n",
    "print(\"Valores únicos em 'gender':\", EDA_humanresources_employee['maritalstatus'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar colunas numéricas para análise \n",
    "numeric_columns = ['sickleavehours', 'vacationhours']\n",
    "\n",
    "# Exibir estatísticas descritivas\n",
    "print(EDA_humanresources_employee[numeric_columns].describe())\n",
    "\n",
    "# Calcular limites para outliers (IQR - Intervalo Interquartil)\n",
    "for col in numeric_columns:\n",
    "    q1 = EDA_humanresources_employee[col].quantile(0.25)\n",
    "    q3 = EDA_humanresources_employee[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    # Exibir os limites\n",
    "    print(f\"\\nColuna: {col}\")\n",
    "    print(f\"Limite inferior: {lower_bound}, Limite superior: {upper_bound}\")\n",
    "    \n",
    "    # Filtrar outliers\n",
    "    outliers = EDA_humanresources_employee[(EDA_humanresources_employee[col] < lower_bound) | (EDA_humanresources_employee[col] > upper_bound)]\n",
    "    print(f\"Outliers detectados ({len(outliers)}):\")\n",
    "    print(outliers[[col]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir regex para validar números (exemplo: apenas dígitos, 9 caracteres)\n",
    "# acrescentei para ver se tinha um padrão, mas não tem\n",
    "regex = r'^\\d{9}$'\n",
    "\n",
    "# Verificar valores inválidos\n",
    "invalid_nationalid = EDA_humanresources_employee[~EDA_humanresources_employee['nationalidnumber'].astype(str).str.match(regex)]\n",
    "print(f\"Valores inválidos em 'nationalidnumber':\\n{invalid_nationalid['nationalidnumber']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um backup do DataFrame tratado\n",
    "EDA_humanresources_employee_bkp_v2 = EDA_humanresources_employee.copy()\n",
    "\n",
    "# Verificar o tamanho do backup e as primeiras linhas\n",
    "print(f\"Backup criado com {len(EDA_humanresources_employee_bkp_v2)} linhas.\")\n",
    "print(EDA_humanresources_employee_bkp_v2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar e documentar colunas existentes\n",
    "print(\"Colunas mantidas no dataset:\", EDA_humanresources_employee.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar colunas binárias esperadas\n",
    "binary_columns = ['currentflag', 'salariedflag']\n",
    "\n",
    "# Verificar valores únicos em colunas binárias\n",
    "for col in binary_columns:\n",
    "    unique_values = EDA_humanresources_employee[col].unique()\n",
    "    print(f\"Valores únicos em '{col}': {unique_values}\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar valores em 'currentflag' e 'salariedflag'\n",
    "print(\"Distribuição de 'currentflag':\")\n",
    "print(EDA_humanresources_employee['currentflag'].value_counts())\n",
    "\n",
    "print(\"\\nDistribuição de 'salariedflag':\")\n",
    "print(EDA_humanresources_employee['salariedflag'].value_counts())\n",
    "\n",
    "\n",
    "#se vale a pena deletar ou não a coluna currentflag, já que só tem 1 valor e é true ?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Verificar se todos os funcionários ativos têm currentflag = True, pois deveria ser false = demitido/desligado\n",
    "print(\"Funcionários ativos errados:\", EDA_humanresources_employee[EDA_humanresources_employee['currentflag'] != True])\n",
    "\n",
    "# 2. Validar datas\n",
    "print(\"Contratações futuras:\", EDA_humanresources_employee[EDA_humanresources_employee['hiredate'] > pd.Timestamp.now()])\n",
    "print(\"Modifieddate antes de hiredate:\", EDA_humanresources_employee[EDA_humanresources_employee['modifieddate'] < EDA_humanresources_employee['hiredate']])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar o formato das colunas de data para atender ao BigQuery\n",
    "EDA_humanresources_employee['modifieddate'] = pd.to_datetime(EDA_humanresources_employee['modifieddate'], errors='coerce').dt.date\n",
    "EDA_humanresources_employee['birthdate'] = pd.to_datetime(EDA_humanresources_employee['birthdate'], errors='coerce').dt.date\n",
    "EDA_humanresources_employee['hiredate'] = pd.to_datetime(EDA_humanresources_employee['hiredate'], errors='coerce').dt.date\n",
    "\n",
    "# Atualizar o dicionário processed_data com o DataFrame ajustado\n",
    "processed_data['humanresources_employee'] = EDA_humanresources_employee\n",
    "\n",
    "# # Exportar tabelas para o BigQuery no formato CSV\n",
    "# for table_name, df_cleaned in processed_data.items():\n",
    "#     output_table = f\"{output_dataset}.{table_name}\"\n",
    "    \n",
    "#     print(f\"Exportando tabela {table_name} para o BigQuery...\")\n",
    "\n",
    "#     # Definir o esquema explicitamente\n",
    "#     schema = [\n",
    "#         bigquery.SchemaField(\"birthdate\", \"DATE\"),\n",
    "#         bigquery.SchemaField(\"businessentityid\", \"INTEGER\"),\n",
    "#         bigquery.SchemaField(\"currentflag\", \"BOOLEAN\"),\n",
    "#         bigquery.SchemaField(\"gender\", \"STRING\"),\n",
    "#         bigquery.SchemaField(\"hiredate\", \"DATE\"),\n",
    "#         bigquery.SchemaField(\"jobtitle\", \"STRING\"),\n",
    "#         bigquery.SchemaField(\"loginid\", \"STRING\"),\n",
    "#         bigquery.SchemaField(\"maritalstatus\", \"STRING\"),\n",
    "#         bigquery.SchemaField(\"modifieddate\", \"DATE\"),\n",
    "#         bigquery.SchemaField(\"nationalidnumber\", \"INTEGER\"),\n",
    "#         bigquery.SchemaField(\"organizationnode\", \"STRING\"),\n",
    "#         bigquery.SchemaField(\"rowguid\", \"STRING\"),\n",
    "#         bigquery.SchemaField(\"salariedflag\", \"BOOLEAN\"),\n",
    "#         bigquery.SchemaField(\"sickleavehours\", \"INTEGER\"),\n",
    "#         bigquery.SchemaField(\"vacationhours\", \"INTEGER\"),\n",
    "#     ]\n",
    "\n",
    "#     job_config = bigquery.LoadJobConfig(\n",
    "#         source_format=bigquery.SourceFormat.CSV,\n",
    "#         skip_leading_rows=0,\n",
    "#         write_disposition=\"WRITE_TRUNCATE\",\n",
    "#         schema=schema,  # Especifica os tipos de dados explicitamente\n",
    "#     )\n",
    "\n",
    "#     # Exportar para o BigQuery\n",
    "#     job = client.load_table_from_dataframe(df_cleaned, output_table, job_config=job_config)\n",
    "#     job.result()\n",
    "\n",
    "#     print(f\"Tabela {table_name} exportada com sucesso para {output_table}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela stg_humanresources_employee exportada com sucesso para desafioadventureworks-446600.raw_data_cleaned.stg_humanresources_employee.\n"
     ]
    }
   ],
   "source": [
    "# Função para ajustar o prefixo do nome da tabela\n",
    "def adjust_table_name(table_name):\n",
    "    # Garante que o nome da tabela não tenha prefixo duplicado\n",
    "    if table_name.startswith(\"stg_stg_\"):\n",
    "        table_name = table_name.replace(\"stg_stg_\", \"stg_\")\n",
    "    return table_name if table_name.startswith((\"fact_\", \"dim_\", \"stg_\")) else f\"stg_{table_name}\"\n",
    "\n",
    "# Remover duplicatas e ajustar os nomes no dicionário\n",
    "processed_data = {\n",
    "    adjust_table_name(table_name): df\n",
    "    for table_name, df in processed_data.items()\n",
    "}\n",
    "\n",
    "# Garantir que apenas tabelas únicas sejam exportadas\n",
    "unique_processed_data = {k: v for k, v in processed_data.items()}\n",
    "\n",
    "# Exportar tabelas para o BigQuery\n",
    "for table_name, df_cleaned in unique_processed_data.items():\n",
    "    output_table = f\"{output_dataset}.{table_name}\"\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"birthdate\", \"DATE\"),\n",
    "        bigquery.SchemaField(\"businessentityid\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"currentflag\", \"BOOLEAN\"),\n",
    "        bigquery.SchemaField(\"gender\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"hiredate\", \"DATE\"),\n",
    "        bigquery.SchemaField(\"jobtitle\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"loginid\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"maritalstatus\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"modifieddate\", \"DATE\"),\n",
    "        bigquery.SchemaField(\"nationalidnumber\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"organizationnode\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"rowguid\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"salariedflag\", \"BOOLEAN\"),\n",
    "        bigquery.SchemaField(\"sickleavehours\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"vacationhours\", \"INTEGER\"),\n",
    "    ]\n",
    "    filtered_schema = [field for field in schema if field.name in df_cleaned.columns]\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        skip_leading_rows=0,\n",
    "        write_disposition=\"WRITE_TRUNCATE\",\n",
    "        schema=filtered_schema,\n",
    "    )\n",
    "    job = client.load_table_from_dataframe(df_cleaned, output_table, job_config=job_config)\n",
    "    job.result()\n",
    "\n",
    "    print(f\"Tabela {table_name} exportada com sucesso para {output_table}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESTATÍSTICA DESCRITIVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar colunas relevantes para análise descritiva\n",
    "cols_para_analise = ['sickleavehours', 'vacationhours']\n",
    "\n",
    "# Garantir que as datas estejam no formato correto\n",
    "EDA_humanresources_employee['hire_year'] = pd.to_datetime(EDA_humanresources_employee['hiredate']).dt.year\n",
    "\n",
    "# Adicionar a nova coluna à lista\n",
    "cols_para_analise.append('hire_year')\n",
    "\n",
    "# Gerar estatísticas descritivas\n",
    "analise_descritiva = EDA_humanresources_employee[cols_para_analise].describe(include='all')\n",
    "\n",
    "# Substituir NaN em colunas numéricas por 0, e em outras colunas por '-'\n",
    "for col in cols_para_analise:\n",
    "    if analise_descritiva[col].dtype.kind in 'ifc':  # Tipos numéricos\n",
    "        analise_descritiva[col] = analise_descritiva[col].fillna(0)\n",
    "    else:\n",
    "        analise_descritiva[col] = analise_descritiva[col].fillna('-')\n",
    "\n",
    "# Gerar estatísticas descritivas\n",
    "resultado_descritivo = analise_descritiva.describe(include='all')\n",
    "\n",
    "print(analise_descritiva)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
