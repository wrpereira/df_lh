{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP INICIAL DO PROJETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#importação das bibliotecase e pacotes necessários para a análise\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas_gbq as gbq\n",
    "import re\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery_storage import BigQueryReadClient\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "\n",
    "# carregar o .env com as credenciais\n",
    "load_dotenv(\"/mnt/c/Users/wrpen/OneDrive/Desktop/df_lh/.env\")\n",
    "\n",
    "\n",
    "# detectar ambiente: como eu estou usando wsl-ubuntu, no VS Code  -  Windows, estava dando conflitos de path\n",
    "if os.name == \"nt\":  # se Windows\n",
    "    credentials_path = r\"C:\\Temp\\desafiolh-445818-3cb0f62cb9ef.json\"\n",
    "else:  # se WSL/Linux\n",
    "    credentials_path = \"/mnt/c/Temp/desafiolh-445818-3cb0f62cb9ef.json\"\n",
    "\n",
    "\n",
    "# parâmetros injetados pelo Papermill ou definidos manualmente, caso não existam no ambiente\n",
    "if 'tables_to_process' not in locals():\n",
    "    tables_to_process = [\n",
    "        \"desafioadventureworks-446600.raw_data.production-productinventory\"       \n",
    "    ]\n",
    "\n",
    "if 'output_dataset' not in locals():\n",
    "    output_dataset = \"desafioadventureworks-446600.raw_data_cleaned\"\n",
    "\n",
    "\n",
    "# configs do cliente BigQuery: input de project e location de acordo com dados no Bigquery\n",
    "credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "client = bigquery.Client(credentials=credentials, project=os.getenv(\"BIGQUERY_PROJECT\"), location=\"us-central1\")\n",
    "\n",
    "\n",
    "#doc: tables_to_process: lista de tabelas que serão processadas\n",
    "#     output_dataset: nome do dataset onde os dados processados serão armazenados, neste caso, raw_data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabelas a processar: ['desafioadventureworks-446600.raw_data.production-productinventory']\n"
     ]
    }
   ],
   "source": [
    "# Print com a tabela que vai ser processada nesse notebook\n",
    "\n",
    "print(\"Tabelas a processar:\", tables_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_data\n",
      "Tabelas disponíveis:\n",
      "humanresources-employee\n",
      "person-address\n",
      "person-businessentity\n",
      "person-person\n",
      "person-stateprovince\n",
      "production-location\n",
      "production-product\n",
      "production-productcategory\n",
      "production-productcosthistory\n",
      "production-productinventory\n",
      "production-productsubcategory\n",
      "purchasing-purchaseorderdetail\n",
      "purchasing-purchaseorderheader\n",
      "purchasing-vendor\n",
      "sales-creditcard\n",
      "sales-customer\n",
      "sales-salesorderdetail\n",
      "sales-salesorderheader\n",
      "sales-salesperson\n",
      "sales-salesterritory\n",
      "sales-store\n"
     ]
    }
   ],
   "source": [
    "# Nome do dataset no Bigquery com os dados brutos (.csv) extraídos pelo Meltano \n",
    "dataset_id = 'raw_data'\n",
    "print(dataset_id)\n",
    "\n",
    "# Lista de tabelas do dataset raw_data no Bigquery\n",
    "tables = client.list_tables('raw_data')\n",
    "print(\"Tabelas disponíveis:\")\n",
    "for table in tables:\n",
    "    print(table.table_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) e Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glossário dos dados:\n",
    "\n",
    "O termo ''doc:'', situado no rodapé de algumas cells, indica algo como:\n",
    "\n",
    "- documentação: documentar decisões, análises e resultados;\n",
    "\n",
    "- abreviações de termos, como bkp, df, entre outros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup inicial do df para realizar a EDA \n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 10000)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "\n",
    "#doc: df = dataframe  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando tabela: desafioadventureworks-446600.raw_data.production-productinventory\n",
      "Lendo os dados do BigQuery...\n",
      "Tabela production_productinventory processada e armazenada com sucesso.\n",
      "Todas as tabelas foram processadas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Dicionário para armazenar os df processados\n",
    "df_processados = {}\n",
    "\n",
    "# Iteração das tabelas e armazenamento em df\n",
    "for input_table in tables_to_process:\n",
    "    print(f\"Processando tabela: {input_table}\")\n",
    "    \n",
    "    table_name = input_table.split(\".\")[-1].replace(\"-\", \"_\")  \n",
    "    \n",
    "    print(\"Lendo os dados do BigQuery...\")\n",
    "    query = f\"SELECT * FROM `{input_table}`\"\n",
    "    table_data = client.query(query).to_dataframe()\n",
    "    \n",
    "    df_processados[table_name] = table_data\n",
    "    print(f\"Tabela {table_name} processada e armazenada com sucesso.\")\n",
    "\n",
    "\n",
    "print(\"Todas as tabelas foram processadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variável criada: production_productinventory\n"
     ]
    }
   ],
   "source": [
    "# Listar todas as variáveis criadas dinamicamente\n",
    "for table_name in df_processados.keys():\n",
    "    print(f\"Variável criada: {table_name}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas: 7\n",
      "Linhas: 13897\n"
     ]
    }
   ],
   "source": [
    "# Atribuir o df a uma variável com nome mais simples\n",
    "production_productinventory = df_processados['production_productinventory']\n",
    "\n",
    "print(f\"Colunas: {production_productinventory.shape[1]}\\nLinhas: {production_productinventory.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicadas ordenadas:\n",
      "       productid  locationid shelf  bin  quantity                               rowguid              modifieddate\n",
      "240            1           1     A    1       408  47a24246-6c43-48eb-968f-025738a8a410 2014-08-08 00:00:00+00:00\n",
      "241            1           1     A    1       408  47a24246-6c43-48eb-968f-025738a8a410 2014-08-08 00:00:00+00:00\n",
      "242            1           1     A    1       408  47a24246-6c43-48eb-968f-025738a8a410 2014-08-08 00:00:00+00:00\n",
      "243            1           1     A    1       408  47a24246-6c43-48eb-968f-025738a8a410 2014-08-08 00:00:00+00:00\n",
      "244            1           1     A    1       408  47a24246-6c43-48eb-968f-025738a8a410 2014-08-08 00:00:00+00:00\n",
      "...          ...         ...   ...  ...       ...                                   ...                       ...\n",
      "11745        999          60   N/A    0       116  aff43c54-af78-4635-8f8a-733a1fc2d085 2013-04-30 00:00:00+00:00\n",
      "11746        999          60   N/A    0       116  aff43c54-af78-4635-8f8a-733a1fc2d085 2013-04-30 00:00:00+00:00\n",
      "11747        999          60   N/A    0       116  aff43c54-af78-4635-8f8a-733a1fc2d085 2013-04-30 00:00:00+00:00\n",
      "13282        999           7   N/A    0        78  61a26db0-bd17-442b-b119-8e5227811b82 2013-04-30 00:00:00+00:00\n",
      "13359        999          60   N/A    0       116  aff43c54-af78-4635-8f8a-733a1fc2d085 2013-04-30 00:00:00+00:00\n",
      "\n",
      "[13897 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Identificar duplicadas\n",
    "duplicadas = production_productinventory[production_productinventory.duplicated(subset=['productid'], keep=False)]\n",
    "\n",
    "# Verificar se existem duplicadas\n",
    "if not duplicadas.empty:\n",
    "    \n",
    "    duplicadas_ordenadas = duplicadas.sort_values(by=['productid', 'modifieddate'])\n",
    "\n",
    "    print(\"duplicadas ordenadas:\")\n",
    "    print(duplicadas_ordenadas)\n",
    "else:\n",
    "    print(\"Não foram encontradas duplicadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linhas após remover duplicatas com base em 'productid' e 'locationid': 1069\n"
     ]
    }
   ],
   "source": [
    "# Remover duplicadas* \n",
    "production_productinventory = production_productinventory.drop_duplicates(subset=['productid', 'locationid'], keep='first')\n",
    "\n",
    "print(f\"Linhas após remover duplicatas com base em 'productid' e 'locationid': {len(production_productinventory)}\")\n",
    "\n",
    "#bkp dos dados brutos\n",
    "raw_data_bkp_sem_duplicadas = production_productinventory.copy()\n",
    "\n",
    "\n",
    "#doc: bkp = backup (cópia)\n",
    "#doc*: mantendo a última ocorrência com base em 'modifieddate', pois ela que indica a data da última modificação nos dados\n",
    "#      Importante, pois se houver erro na ingestão (duplicação), mantém os dados íntegros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       productid  locationid shelf  bin  quantity                               rowguid              modifieddate\n",
      "1368           1           6     B    5       324  d4544d7d-caf5-46b3-ab22-5718dcc26b5e 2014-08-08 00:00:00+00:00\n",
      "720            1          50     A    5       353  bff7dc60-96a8-43ca-81a7-d6d2ed3000a8 2014-08-08 00:00:00+00:00\n",
      "240            1           1     A    1       408  47a24246-6c43-48eb-968f-025738a8a410 2014-08-08 00:00:00+00:00\n",
      "924            2           1     A    2       427  f407c07a-ca14-4684-a02c-608bd00c2233 2014-08-08 00:00:00+00:00\n",
      "1332           2           6     B    1       318  ca1ff2f4-48fb-4960-8d92-3940b633e4c1 2014-08-08 00:00:00+00:00\n",
      "...          ...         ...   ...  ...       ...                                   ...                       ...\n",
      "10452        997           7   N/A    0       123  540b82be-09e5-4e99-9b7b-835ffab06950 2013-04-30 00:00:00+00:00\n",
      "10392        998          60   N/A    0        56  5021e7ea-ce96-433c-860a-b9e57a45ef03 2013-04-30 00:00:00+00:00\n",
      "11820        998           7   N/A    0        99  b859a235-a1cc-48d7-92bb-10d0c2886e4e 2013-04-30 00:00:00+00:00\n",
      "10680        999           7   N/A    0        78  61a26db0-bd17-442b-b119-8e5227811b82 2013-04-30 00:00:00+00:00\n",
      "11736        999          60   N/A    0       116  aff43c54-af78-4635-8f8a-733a1fc2d085 2013-04-30 00:00:00+00:00\n",
      "\n",
      "[1069 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Ordenar e exibir o df por 'productid'\n",
    "production_productinventory = production_productinventory.sort_values(by=['productid'])\n",
    "\n",
    "print(production_productinventory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coluna 'productid': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'productid'.\n",
      "\n",
      "Coluna 'locationid': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'locationid'.\n",
      "\n",
      "Coluna 'shelf': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'shelf'.\n",
      "\n",
      "Coluna 'bin': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'bin'.\n",
      "\n",
      "Coluna 'quantity': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'quantity'.\n",
      "\n",
      "Coluna 'rowguid': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'rowguid'.\n",
      "\n",
      "Coluna 'modifieddate': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'modifieddate'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterar por todas as colunas do df, para verificar valores ausentes\n",
    "\n",
    "# Verificar valores ausentes na coluna\n",
    "for column in production_productinventory.columns:   \n",
    "    missing_rows = production_productinventory[production_productinventory[column].isnull()]\n",
    "    print(f\"Coluna '{column}': {missing_rows.shape[0]} linhas ausentes.\")\n",
    "    \n",
    "# Mostrar as primeiras linhas ausentes, se preciso for, limitar o head() para dar menos outputs ou limitar os outputs\n",
    "    if not missing_rows.empty:\n",
    "        print(f\"Exibindo as primeiras linhas com valores ausentes em '{column}':\")\n",
    "        print(missing_rows.head(), \"\\n\")\n",
    "    else:\n",
    "        print(f\"Nenhuma linha com valores ausentes em '{column}'.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores únicos incluindo NaN:\n",
      "productid        432\n",
      "locationid        14\n",
      "shelf             21\n",
      "bin               62\n",
      "quantity         343\n",
      "rowguid         1069\n",
      "modifieddate      24\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Valores únicos por coluna, para verificar se colunas como flags, normalmente booleanas, possuem apenas 1 ou 2 valores.\n",
    "\n",
    "valores_unicos = production_productinventory.nunique(dropna=False)\n",
    "\n",
    "print(\"Valores únicos incluindo NaN:\")\n",
    "print(valores_unicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1069 entries, 1368 to 11736\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype              \n",
      "---  ------        --------------  -----              \n",
      " 0   productid     1069 non-null   Int64              \n",
      " 1   locationid    1069 non-null   Int64              \n",
      " 2   shelf         1069 non-null   object             \n",
      " 3   bin           1069 non-null   Int64              \n",
      " 4   quantity      1069 non-null   Int64              \n",
      " 5   rowguid       1069 non-null   object             \n",
      " 6   modifieddate  1069 non-null   datetime64[us, UTC]\n",
      "dtypes: Int64(4), datetime64[us, UTC](1), object(2)\n",
      "memory usage: 71.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#verificar informações do df\n",
    "production_productinventory.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       quantity\n",
      "count   1069.00\n",
      "mean     314.29\n",
      "std      189.85\n",
      "min        0.00\n",
      "25%      140.00\n",
      "50%      299.00\n",
      "75%      481.00\n",
      "max      924.00\n",
      "\n",
      "Coluna: quantity\n",
      "Limite inferior: -371.5, Limite superior: 992.5\n",
      "Outliers detectados (0):\n",
      "Empty DataFrame\n",
      "Columns: [quantity]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Variáveis quantitativas*: estatísticas descritivas para verificar se ainda há o que ser feito antes de exportar os dados ao BigQuery\n",
    "\n",
    "# Identificar colunas numéricas para análise de outliers\n",
    "numeric_columns = ['quantity']\n",
    "\n",
    "# Estatísticas Descritivas das colunas numéricas*\n",
    "print(production_productinventory[numeric_columns].describe())\n",
    "\n",
    "# Cálculo de limites para outliers (IQR)**\n",
    "for col in numeric_columns:\n",
    "    q1 = production_productinventory[col].quantile(0.25)\n",
    "    q3 = production_productinventory[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    # Exibir os limites\n",
    "    print(f\"\\nColuna: {col}\")\n",
    "    print(f\"Limite inferior: {lower_bound}, Limite superior: {upper_bound}\")\n",
    "    \n",
    "    # Detecção e Análise de Outliers***\n",
    "    outliers = production_productinventory[(production_productinventory[col] < lower_bound) | (production_productinventory[col] > upper_bound)]\n",
    "    print(f\"Outliers detectados ({len(outliers)}):\")\n",
    "    print(outliers[[col]])\n",
    "\n",
    "\n",
    "#doc*: variáveis quantitativas são um tipo de dado que pode ser representado por números e medidas objetivas\n",
    "#doc*: realizar estatísticas descritivas para entender a centralidade e variação dos dados (valores médios, mínimos, máximos, etc.)\n",
    "#doc**: calcular limites para identificar outliers (valores extremos que podem indicar erros ou casos excepcionais nos dados)\n",
    "#doc***: verificar a existência de outliers para decidir ações como remoção, substituição ou tratamento, garantindo qualidade dos dados\n",
    "#doc****: as colunas analisadas não apresentam outliers, pois os dados estão dentro dos limites esperados, sugerindo que não há necessidade de tratamento adicional para valores extremos. Isso indica boa qualidade dos dados para essas variáveis e que elas estão prontas para serem exportadas ou utilizadas em análises e modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualizar o dicionário df_processados com o df ajustado\n",
    "df_processados['production_productinventory'] = production_productinventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      productid  locationid shelf  bin  quantity                               rowguid              modifieddate\n",
      "1368          1           6     B    5       324  D4544D7D-CAF5-46B3-AB22-5718DCC26B5E 2014-08-08 00:00:00+00:00\n",
      "720           1          50     A    5       353  BFF7DC60-96A8-43CA-81A7-D6D2ED3000A8 2014-08-08 00:00:00+00:00\n",
      "240           1           1     A    1       408  47A24246-6C43-48EB-968F-025738A8A410 2014-08-08 00:00:00+00:00\n",
      "924           2           1     A    2       427  F407C07A-CA14-4684-A02C-608BD00C2233 2014-08-08 00:00:00+00:00\n",
      "1332          2           6     B    1       318  CA1FF2F4-48FB-4960-8D92-3940B633E4C1 2014-08-08 00:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# Padronizar colunas com valores textuais\n",
    "production_productinventory['shelf'] = production_productinventory['shelf'].str.strip().str.upper()\n",
    "production_productinventory['rowguid'] = production_productinventory['rowguid'].str.strip().str.upper()\n",
    "\n",
    "print(production_productinventory.head())\n",
    "\n",
    "#doc: padronizar as strings nessa etapa, contribui para a execução das demais etapas do pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela production_productinventory exportada com sucesso para desafioadventureworks-446600.raw_data_cleaned.production_productinventory.\n"
     ]
    }
   ],
   "source": [
    "# Garantir que apenas tabelas únicas sejam exportadas\n",
    "unique_df_processados = {k: v for k, v in df_processados.items()}\n",
    "\n",
    "# Exportar tabelas para o BigQuery\n",
    "for table_name, df_cleaned in unique_df_processados.items():\n",
    " \n",
    "    output_table = f\"{output_dataset}.{table_name}\"\n",
    "   \n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_TRUNCATE\"  \n",
    "    )\n",
    "    \n",
    "    job = client.load_table_from_dataframe(df_cleaned, output_table, job_config=job_config)\n",
    "    job.result()\n",
    "\n",
    "    print(f\"Tabela {table_name} exportada com sucesso para {output_table}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
