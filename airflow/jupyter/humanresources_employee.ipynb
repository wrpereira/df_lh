{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP INICIAL DO PROJETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#importação das bibliotecase e pacotes necessários para a análise\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas_gbq as gbq\n",
    "import re\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery_storage import BigQueryReadClient\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Carrega o .env: onde estão as credenciais do projeto/repositório\n",
    "load_dotenv(\"/mnt/c/Users/wrpen/OneDrive/Desktop/df_lh/.env\")\n",
    "\n",
    "# Detectar ambiente: como eu estou usando wsl-ubuntu, no VS Code  -  Windows, estava dando conflitos de path\n",
    "if os.name == \"nt\":  # se Windows\n",
    "    credentials_path = r\"C:\\Temp\\desafiolh-445818-3cb0f62cb9ef.json\"\n",
    "else:  # se WSL/Linux\n",
    "    credentials_path = \"/mnt/c/Temp/desafiolh-445818-3cb0f62cb9ef.json\"\n",
    "\n",
    "# Parâmetros injetados pelo Papermill ou definidos manualmente, caso não existam no ambiente\n",
    "# Tables_to_process: lista de tabelas que serão processadas\n",
    "# Output_dataset: nome do dataset onde os dados processados serão armazenados, neste caso, raw_data_cleaned\n",
    "if 'tables_to_process' not in locals():\n",
    "    tables_to_process = [\n",
    "        \"desafioadventureworks-446600.raw_data.humanresources-employee\"       \n",
    "    ]\n",
    "\n",
    "if 'output_dataset' not in locals():\n",
    "    output_dataset = \"desafioadventureworks-446600.raw_data_cleaned\"\n",
    "\n",
    "# Configs do cliente BigQuery: input de project e location de acordo com dados no Bigquery\n",
    "credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "client = bigquery.Client(credentials=credentials, project=os.getenv(\"BIGQUERY_PROJECT\"), location=\"us-central1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabelas a processar: ['desafioadventureworks-446600.raw_data.humanresources-employee']\n"
     ]
    }
   ],
   "source": [
    "# Print com a tabela que vai ser processada nesse notebook\n",
    "\n",
    "print(\"Tabelas a processar:\", tables_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_data\n",
      "Tabelas disponíveis:\n",
      "humanresources-employee\n",
      "person-address\n",
      "person-businessentity\n",
      "person-person\n",
      "production-location\n",
      "production-product\n",
      "production-productcategory\n",
      "production-productinventory\n",
      "production-productsubcategory\n",
      "sales-customer\n",
      "sales-salesorderdetail\n",
      "sales-salesorderheader\n",
      "sales-salesterritory\n",
      "sales-store\n"
     ]
    }
   ],
   "source": [
    "# Nome do dataset no Bigquery com os dados brutos (.csv) extraídos pelo Meltano \n",
    "dataset_id = 'raw_data'\n",
    "print(dataset_id)\n",
    "\n",
    "# Lista de tabelas do dataset raw_data no Bigquery\n",
    "tables = client.list_tables('raw_data')\n",
    "print(\"Tabelas disponíveis:\")\n",
    "for table in tables:\n",
    "    print(table.table_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) e Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glossário dos dados:\n",
    "\n",
    "df = dataframe  \n",
    "\n",
    "bkp = backup (cópia)\n",
    "\n",
    "doc = documentação / documentar decisões ou análises\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração para que o df exiba todas as colunas e todas as linhas completas, e também, exiba o formato numérico com 2 dígitos após a vírgula\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 10000)\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando tabela: desafioadventureworks-446600.raw_data.humanresources-employee\n",
      "Lendo os dados do BigQuery...\n",
      "Tabela humanresources_employee processada e armazenada com sucesso.\n",
      "Todas as tabelas foram processadas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Dicionário para armazenar os df processados\n",
    "df_processados = {}\n",
    "\n",
    "# Iteração das tabelas e armazenamento em df\n",
    "for input_table in tables_to_process:\n",
    "    print(f\"Processando tabela: {input_table}\")\n",
    "    \n",
    "    # Nome da tabela com substituição de '-' por '_'\n",
    "    table_name = input_table.split(\".\")[-1].replace(\"-\", \"_\")  \n",
    "    \n",
    "    # Ler os dados da tabela do BigQuery para um df\n",
    "    print(\"Lendo os dados do BigQuery...\")\n",
    "    query = f\"SELECT * FROM `{input_table}`\"\n",
    "    table_data = client.query(query).to_dataframe()\n",
    "    \n",
    "    # Armazenar o df no dicionário\n",
    "    df_processados[table_name] = table_data\n",
    "    print(f\"Tabela {table_name} processada e armazenada com sucesso.\")\n",
    "\n",
    "# Print de validação\n",
    "print(\"Todas as tabelas foram processadas com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variável criada: humanresources_employee\n"
     ]
    }
   ],
   "source": [
    "# Listar todas as variáveis criadas dinamicamente\n",
    "for table_name in df_processados.keys():\n",
    "    print(f\"Variável criada: {table_name}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas: 15\n",
      "Linhas: 1740\n"
     ]
    }
   ],
   "source": [
    "# Atribuir o df a uma variável com nome mais simples\n",
    "humanresources_employee = df_processados['humanresources_employee']\n",
    "\n",
    "print(f\"Colunas: {humanresources_employee.shape[1]}\\nLinhas: {humanresources_employee.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicatas ordenadas:\n",
      "      businessentityid  nationalidnumber                  loginid                 jobtitle   birthdate maritalstatus gender    hiredate  salariedflag  vacationhours  sickleavehours  currentflag                               rowguid              modifieddate organizationnode\n",
      "1415                 1         295847284     adventure-works\\ken0  Chief Executive Officer  1969-01-29             S      M  2009-01-14          True             99              69         True  f01251e5-96a3-448d-981e-0f99d789110d 2014-06-30 00:00:00+00:00                /\n",
      "1416                 1         295847284     adventure-works\\ken0  Chief Executive Officer  1969-01-29             S      M  2009-01-14          True             99              69         True  f01251e5-96a3-448d-981e-0f99d789110d 2014-06-30 00:00:00+00:00                /\n",
      "1417                 1         295847284     adventure-works\\ken0  Chief Executive Officer  1969-01-29             S      M  2009-01-14          True             99              69         True  f01251e5-96a3-448d-981e-0f99d789110d 2014-06-30 00:00:00+00:00                /\n",
      "1418                 1         295847284     adventure-works\\ken0  Chief Executive Officer  1969-01-29             S      M  2009-01-14          True             99              69         True  f01251e5-96a3-448d-981e-0f99d789110d 2014-06-30 00:00:00+00:00                /\n",
      "1419                 1         295847284     adventure-works\\ken0  Chief Executive Officer  1969-01-29             S      M  2009-01-14          True             99              69         True  f01251e5-96a3-448d-981e-0f99d789110d 2014-06-30 00:00:00+00:00                /\n",
      "...                ...               ...                      ...                      ...         ...           ...    ...         ...           ...            ...             ...          ...                                   ...                       ...              ...\n",
      "456                290         134219713  adventure-works\\ranjit0     Sales Representative  1975-09-30             S      M  2012-05-30          True             34              37         True  604213f9-dd0f-43b4-bdd2-c96e93d3f4bf 2014-06-30 00:00:00+00:00          /6/3/3/\n",
      "457                290         134219713  adventure-works\\ranjit0     Sales Representative  1975-09-30             S      M  2012-05-30          True             34              37         True  604213f9-dd0f-43b4-bdd2-c96e93d3f4bf 2014-06-30 00:00:00+00:00          /6/3/3/\n",
      "458                290         134219713  adventure-works\\ranjit0     Sales Representative  1975-09-30             S      M  2012-05-30          True             34              37         True  604213f9-dd0f-43b4-bdd2-c96e93d3f4bf 2014-06-30 00:00:00+00:00          /6/3/3/\n",
      "459                290         134219713  adventure-works\\ranjit0     Sales Representative  1975-09-30             S      M  2012-05-30          True             34              37         True  604213f9-dd0f-43b4-bdd2-c96e93d3f4bf 2014-06-30 00:00:00+00:00          /6/3/3/\n",
      "1638               290         134219713  adventure-works\\ranjit0     Sales Representative  1975-09-30             S      M  2012-05-30          True             34              37         True  604213f9-dd0f-43b4-bdd2-c96e93d3f4bf 2014-06-30 00:00:00+00:00          /6/3/3/\n",
      "\n",
      "[1740 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "# Identificar duplicatas com base em 'businessentityid'\n",
    "duplicatas = humanresources_employee[\n",
    "    humanresources_employee.duplicated(subset=['businessentityid'], keep=False)\n",
    "]\n",
    "\n",
    "# Verificar se existem duplicatas\n",
    "if not duplicatas.empty:\n",
    "    # Ordenar duplicatas por 'businessentityid' e 'modifieddate'\n",
    "    duplicatas_ordenadas = duplicatas.sort_values(by=['businessentityid', 'modifieddate'])\n",
    "\n",
    "    # Exibir duplicatas ordenadas\n",
    "    print(\"Duplicatas ordenadas:\")\n",
    "    print(duplicatas_ordenadas)\n",
    "else:\n",
    "    print(\"Não foram encontradas duplicatas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linhas após remover duplicatas (baseando-se na última 'modifieddate'): 290\n"
     ]
    }
   ],
   "source": [
    "# Remover duplicatas mantendo a última ocorrência com base em 'modifieddate', pois ela que indica a data da última modificação nos dados\n",
    "# Importante, pois se houver erro na ingestão (duplicação), mantém os dados integros.\n",
    "\n",
    "humanresources_employee = humanresources_employee.drop_duplicates(subset=['businessentityid'], keep='last')\n",
    "\n",
    "print(f\"Linhas após remover duplicatas (baseando-se na última 'modifieddate'): {len(humanresources_employee)}\")\n",
    "\n",
    "#bkp dos dados brutos\n",
    "raw_data_bkp_2_sem_duplicatas = humanresources_employee.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      businessentityid  nationalidnumber                   loginid                       jobtitle   birthdate maritalstatus gender    hiredate  salariedflag  vacationhours  sickleavehours  currentflag                               rowguid              modifieddate organizationnode\n",
      "1738                 1         295847284      adventure-works\\ken0        Chief Executive Officer  1969-01-29             S      M  2009-01-14          True             99              69         True  f01251e5-96a3-448d-981e-0f99d789110d 2014-06-30 00:00:00+00:00                /\n",
      "1598                 2         245797967    adventure-works\\terri0  Vice President of Engineering  1971-08-01             S      F  2008-01-31          True              1              20         True  45e8f437-670d-4409-93cb-f9424a40d6ee 2014-06-30 00:00:00+00:00              /1/\n",
      "1453                 3         509647174  adventure-works\\roberto0            Engineering Manager  1974-11-12             M      M  2007-11-11          True              2              21         True  9bbbfb2c-efbb-4217-9ab7-f97689328841 2014-06-30 00:00:00+00:00            /1/1/\n",
      "1656                 4         112457891      adventure-works\\rob0           Senior Tool Designer  1974-12-23             S      M  2007-12-05         False             48              80         True  59747955-87b8-443f-8ed4-f8ad3afdf3a9 2014-06-30 00:00:00+00:00          /1/1/1/\n",
      "1458                 5         695256908     adventure-works\\gail0                Design Engineer  1952-09-27             M      F  2008-01-06          True              5              22         True  ec84ae09-f9b8-4a15-b4a9-6ccbab919b08 2014-06-30 00:00:00+00:00          /1/1/2/\n",
      "...                ...               ...                       ...                            ...         ...           ...    ...         ...           ...            ...             ...          ...                                   ...                       ...              ...\n",
      "1643               286         758596752     adventure-works\\lynn0           Sales Representative  1977-02-14             S      F  2013-05-30          True             36              38         True  4a9a8407-a680-4a6b-8d03-511cb58f9a8a 2014-06-30 00:00:00+00:00          /6/2/1/\n",
      "1484               287         982310417      adventure-works\\amy0         European Sales Manager  1957-09-20             M      F  2012-04-16          True             21              30         True  66d66445-ee78-4676-9e66-0e22d6109a92 2014-06-30 00:00:00+00:00            /6/3/\n",
      "1640               288         954276278   adventure-works\\rachel0           Sales Representative  1975-07-09             S      F  2013-05-30          True             35              37         True  b9bf7741-e0ca-4f37-acde-a4f78c6d03e9 2014-06-30 00:00:00+00:00          /6/3/1/\n",
      "1504               289         668991357      adventure-works\\jae0           Sales Representative  1968-03-17             M      F  2012-05-30          True             37              38         True  723a5921-d8a1-4659-9bc4-13c4cf7c9c91 2014-06-30 00:00:00+00:00          /6/3/2/\n",
      "1638               290         134219713   adventure-works\\ranjit0           Sales Representative  1975-09-30             S      M  2012-05-30          True             34              37         True  604213f9-dd0f-43b4-bdd2-c96e93d3f4bf 2014-06-30 00:00:00+00:00          /6/3/3/\n",
      "\n",
      "[290 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "# Ordenar e exibir o df por 'businessentityid'\n",
    "humanresources_employee = humanresources_employee.sort_values(by=['businessentityid'])\n",
    "\n",
    "print(humanresources_employee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 290 entries, 1738 to 1638\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Non-Null Count  Dtype              \n",
      "---  ------            --------------  -----              \n",
      " 0   businessentityid  290 non-null    Int64              \n",
      " 1   nationalidnumber  290 non-null    Int64              \n",
      " 2   loginid           290 non-null    object             \n",
      " 3   jobtitle          290 non-null    object             \n",
      " 4   birthdate         290 non-null    dbdate             \n",
      " 5   maritalstatus     290 non-null    object             \n",
      " 6   gender            290 non-null    object             \n",
      " 7   hiredate          290 non-null    dbdate             \n",
      " 8   salariedflag      290 non-null    boolean            \n",
      " 9   vacationhours     290 non-null    Int64              \n",
      " 10  sickleavehours    290 non-null    Int64              \n",
      " 11  currentflag       290 non-null    boolean            \n",
      " 12  rowguid           290 non-null    object             \n",
      " 13  modifieddate      290 non-null    datetime64[us, UTC]\n",
      " 14  organizationnode  290 non-null    object             \n",
      "dtypes: Int64(4), boolean(2), datetime64[us, UTC](1), dbdate(2), object(6)\n",
      "memory usage: 34.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Verificar se as colunas de DATAS têm valores nulos (NaN em pandas), pois não pode object, para facilitar manipulação dos dados\n",
    "print(humanresources_employee.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coluna 'businessentityid': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'businessentityid'.\n",
      "\n",
      "Coluna 'nationalidnumber': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'nationalidnumber'.\n",
      "\n",
      "Coluna 'loginid': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'loginid'.\n",
      "\n",
      "Coluna 'jobtitle': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'jobtitle'.\n",
      "\n",
      "Coluna 'birthdate': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'birthdate'.\n",
      "\n",
      "Coluna 'maritalstatus': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'maritalstatus'.\n",
      "\n",
      "Coluna 'gender': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'gender'.\n",
      "\n",
      "Coluna 'hiredate': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'hiredate'.\n",
      "\n",
      "Coluna 'salariedflag': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'salariedflag'.\n",
      "\n",
      "Coluna 'vacationhours': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'vacationhours'.\n",
      "\n",
      "Coluna 'sickleavehours': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'sickleavehours'.\n",
      "\n",
      "Coluna 'currentflag': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'currentflag'.\n",
      "\n",
      "Coluna 'rowguid': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'rowguid'.\n",
      "\n",
      "Coluna 'modifieddate': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'modifieddate'.\n",
      "\n",
      "Coluna 'organizationnode': 0 linhas ausentes.\n",
      "Nenhuma linha com valores ausentes em 'organizationnode'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterar por todas as colunas do df, para verificar valores ausentes\n",
    "\n",
    "# Verificar valores ausentes na coluna\n",
    "for column in humanresources_employee.columns:   \n",
    "    missing_rows = humanresources_employee[humanresources_employee[column].isnull()]\n",
    "    print(f\"Coluna '{column}': {missing_rows.shape[0]} linhas ausentes.\")\n",
    "    \n",
    "# Mostrar as primeiras linhas ausentes, se preciso for, limitar o head() para dar menos outputs ou limitar os outputs\n",
    "    if not missing_rows.empty:\n",
    "        print(f\"Exibindo as primeiras linhas com valores ausentes em '{column}':\")\n",
    "        print(missing_rows.head(), \"\\n\")\n",
    "    else:\n",
    "        print(f\"Nenhuma linha com valores ausentes em '{column}'.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores únicos incluindo NaN:\n",
      "businessentityid    290\n",
      "nationalidnumber    290\n",
      "loginid             290\n",
      "jobtitle             67\n",
      "birthdate           275\n",
      "maritalstatus         2\n",
      "gender                2\n",
      "hiredate            164\n",
      "salariedflag          2\n",
      "vacationhours       100\n",
      "sickleavehours       51\n",
      "currentflag           1\n",
      "rowguid             290\n",
      "modifieddate          2\n",
      "organizationnode    290\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Valores únicos por coluna, para verificar se colunas como flags, normalmente booleanas, possuem apenas 1 ou 2 valores.\n",
    "\n",
    "valores_unicos = humanresources_employee.nunique(dropna=False)\n",
    "\n",
    "print(\"Valores únicos incluindo NaN:\")\n",
    "print(valores_unicos)\n",
    "\n",
    "#doc: currentflag possue somente 1 valor, o que indica que pode ser somente valores True ou False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      businessentityid  nationalidnumber                   loginid                       jobtitle   birthdate maritalstatus gender    hiredate  salariedflag  vacationhours  sickleavehours  currentflag                               rowguid              modifieddate organizationnode\n",
      "1738                 1         295847284      ADVENTURE-WORKS\\KEN0        CHIEF EXECUTIVE OFFICER  1969-01-29             S      M  2009-01-14          True             99              69         True  F01251E5-96A3-448D-981E-0F99D789110D 2014-06-30 00:00:00+00:00                /\n",
      "1598                 2         245797967    ADVENTURE-WORKS\\TERRI0  VICE PRESIDENT OF ENGINEERING  1971-08-01             S      F  2008-01-31          True              1              20         True  45E8F437-670D-4409-93CB-F9424A40D6EE 2014-06-30 00:00:00+00:00              /1/\n",
      "1453                 3         509647174  ADVENTURE-WORKS\\ROBERTO0            ENGINEERING MANAGER  1974-11-12             M      M  2007-11-11          True              2              21         True  9BBBFB2C-EFBB-4217-9AB7-F97689328841 2014-06-30 00:00:00+00:00            /1/1/\n",
      "1656                 4         112457891      ADVENTURE-WORKS\\ROB0           SENIOR TOOL DESIGNER  1974-12-23             S      M  2007-12-05         False             48              80         True  59747955-87B8-443F-8ED4-F8AD3AFDF3A9 2014-06-30 00:00:00+00:00          /1/1/1/\n",
      "1458                 5         695256908     ADVENTURE-WORKS\\GAIL0                DESIGN ENGINEER  1952-09-27             M      F  2008-01-06          True              5              22         True  EC84AE09-F9B8-4A15-B4A9-6CCBAB919B08 2014-06-30 00:00:00+00:00          /1/1/2/\n"
     ]
    }
   ],
   "source": [
    "# Padronizar textos em title (ex: Production Supervisor) ou upper (ex:M)\n",
    "humanresources_employee['jobtitle'] = humanresources_employee['jobtitle'].str.strip().str.upper()\n",
    "humanresources_employee['gender'] = humanresources_employee['gender'].str.strip().str.upper()\n",
    "humanresources_employee['maritalstatus'] = humanresources_employee['maritalstatus'].str.strip().str.upper()\n",
    "humanresources_employee['loginid'] = humanresources_employee['loginid'].str.strip().str.upper()\n",
    "humanresources_employee['rowguid'] = humanresources_employee['rowguid'].str.strip().str.upper()\n",
    "\n",
    "print(humanresources_employee.head())\n",
    "\n",
    "\n",
    "#doc: padronizar as strings nessa etapa, contribui para a execução das demais etapas do pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores únicos em 'currentflag': <BooleanArray>\n",
      "[True]\n",
      "Length: 1, dtype: boolean\n",
      "Distribuição de 'currentflag':\n",
      "currentflag\n",
      "True    290\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Valores únicos em 'salariedflag': <BooleanArray>\n",
      "[True, False]\n",
      "Length: 2, dtype: boolean\n",
      "Distribuição de 'salariedflag':\n",
      "salariedflag\n",
      "False    238\n",
      "True      52\n",
      "Name: count, dtype: Int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Listar colunas binárias esperadas e verificar valores únicos e suas distribuições\n",
    "binary_columns = ['currentflag', 'salariedflag']\n",
    "\n",
    "for col in binary_columns:\n",
    "    unique_values = humanresources_employee[col].unique()\n",
    "    print(f\"Valores únicos em '{col}': {unique_values}\")\n",
    "    print(f\"Distribuição de '{col}':\")\n",
    "    print(humanresources_employee[col].value_counts())\n",
    "    print()\n",
    "\n",
    "\n",
    "#doc: garantir que colunas binárias contenham apenas valores esperados, no caso True ou False e identificar anomalias verificando os valores únicos em cada coluna; valores fora do padrão binário, facilitando a validação e correção\n",
    "#doc: currentflag só tem True, isso indica que a coluna é constante e não agrega informações úteis para a análise, pois não há variabilidade nos dados, ao contrario de salariedflag, que é uma coluna válida para análise, já que possui variabilidade e pode ser utilizada para segmentar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funcionários ativos errados: Empty DataFrame\n",
      "Columns: [businessentityid, nationalidnumber, loginid, jobtitle, birthdate, maritalstatus, gender, hiredate, salariedflag, vacationhours, sickleavehours, currentflag, rowguid, modifieddate, organizationnode]\n",
      "Index: []\n",
      "Contratações futuras: Empty DataFrame\n",
      "Columns: [businessentityid, nationalidnumber, loginid, jobtitle, birthdate, maritalstatus, gender, hiredate, salariedflag, vacationhours, sickleavehours, currentflag, rowguid, modifieddate, organizationnode]\n",
      "Index: []\n",
      "Modifieddate antes de hiredate: Empty DataFrame\n",
      "Columns: [businessentityid, nationalidnumber, loginid, jobtitle, birthdate, maritalstatus, gender, hiredate, salariedflag, vacationhours, sickleavehours, currentflag, rowguid, modifieddate, organizationnode]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Verificar se há funcionários ativos com `false`*\n",
    "print(\"Funcionários ativos errados:\", humanresources_employee[humanresources_employee['currentflag'] != True])\n",
    "\n",
    "# Validar datas\n",
    "# Converter 'hiredate' e 'modifieddate' para datetime sem timezone\n",
    "hiredate = pd.to_datetime(humanresources_employee['hiredate'], errors='coerce').dt.tz_localize(None)\n",
    "modifieddate = pd.to_datetime(humanresources_employee['modifieddate'], errors='coerce').dt.tz_localize(None)\n",
    "\n",
    "# Contratações futuras**\n",
    "print(\"Contratações futuras:\", humanresources_employee[hiredate > pd.Timestamp.now()])\n",
    "\n",
    "# Modifieddate antes de hiredate***\n",
    "print(\"Modifieddate antes de hiredate:\", humanresources_employee[modifieddate < hiredate])\n",
    "\n",
    "\n",
    "#doc*: funcionários desligados / demitidos (False) não devem ser tratados como ativos no sistema, evitando inconsistências\n",
    "#doc**:datas futuras indicam erros nos registros, pois contratações devem ocorrer em datas passadas\n",
    "#doc***: um registro não pode ser modificado antes da contratação, pois seria um erro lógico nos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualizar o dicionário df_processados com o df ajustado\n",
    "df_processados['humanresources_employee'] = humanresources_employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sickleavehours  vacationhours\n",
      "count          290.00         290.00\n",
      "mean            45.31          50.61\n",
      "std             14.54          28.79\n",
      "min             20.00           0.00\n",
      "25%             33.00          26.25\n",
      "50%             46.00          51.00\n",
      "75%             58.00          75.00\n",
      "max             80.00          99.00\n",
      "\n",
      "Coluna: sickleavehours\n",
      "Limite inferior: -4.5, Limite superior: 95.5\n",
      "Outliers detectados (0):\n",
      "Empty DataFrame\n",
      "Columns: [sickleavehours]\n",
      "Index: []\n",
      "\n",
      "Coluna: vacationhours\n",
      "Limite inferior: -46.875, Limite superior: 148.125\n",
      "Outliers detectados (0):\n",
      "Empty DataFrame\n",
      "Columns: [vacationhours]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Estatísticas descritivas para verificar se ainda há o que ser feito antes de exportar os dados ao BigQuery\n",
    "\n",
    "# Identificar colunas numéricas para análise de outliers\n",
    "numeric_columns = ['sickleavehours', 'vacationhours']\n",
    "\n",
    "# Estatísticas Descritivas das colunas numéricas*\n",
    "print(humanresources_employee[numeric_columns].describe())\n",
    "\n",
    "# Cálculo de limites para outliers (IQR)**\n",
    "for col in numeric_columns:\n",
    "    q1 = humanresources_employee[col].quantile(0.25)\n",
    "    q3 = humanresources_employee[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    # Exibir os limites\n",
    "    print(f\"\\nColuna: {col}\")\n",
    "    print(f\"Limite inferior: {lower_bound}, Limite superior: {upper_bound}\")\n",
    "    \n",
    "    # Detecção e Análise de Outliers***\n",
    "    outliers = humanresources_employee[(humanresources_employee[col] < lower_bound) | (humanresources_employee[col] > upper_bound)]\n",
    "    print(f\"Outliers detectados ({len(outliers)}):\")\n",
    "    print(outliers[[col]])\n",
    "\n",
    "\n",
    "#doc: as colunas analisadas não apresentam outliers, pois os dados estão dentro dos limites esperados, sugerindo que não há necessidade de tratamento adicional para valores extremos. Isso indica boa qualidade dos dados para essas variáveis e que elas estão prontas para serem exportadas ou utilizadas em análises e modelos\n",
    "#doc*: realizar estatísticas descritivas para entender a centralidade e variação dos dados (valores médios, mínimos, máximos, etc.)\n",
    "#doc**: calcular limites para identificar outliers (valores extremos que podem indicar erros ou casos excepcionais nos dados)\n",
    "#doc***: verificar a existência de outliers para decidir ações como remoção, substituição ou tratamento, garantindo qualidade dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela humanresources_employee exportada com sucesso para desafioadventureworks-446600.raw_data_cleaned.humanresources_employee.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Garantir que apenas tabelas únicas sejam exportadas\n",
    "unique_df_processados = {k: v for k, v in df_processados.items()}\n",
    "\n",
    "# Exportar tabelas para o BigQuery\n",
    "for table_name, df_cleaned in unique_df_processados.items():\n",
    "    # Nome da tabela no BigQuery\n",
    "    output_table = f\"{output_dataset}.{table_name}\"\n",
    "\n",
    "    # Configurar job de exportação\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_TRUNCATE\"  # Substituir dados existentes\n",
    "    )\n",
    "    \n",
    "    # Exportar DataFrame para o BigQuery\n",
    "    job = client.load_table_from_dataframe(df_cleaned, output_table, job_config=job_config)\n",
    "    job.result()\n",
    "\n",
    "    print(f\"Tabela {table_name} exportada com sucesso para {output_table}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
